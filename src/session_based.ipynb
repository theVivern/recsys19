{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from operator import add\n",
    "import json\n",
    "from collections import Counter\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, IDF, CountVectorizer, ChiSqSelector, StringIndexer, Normalizer\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, CrossValidatorModel, ParamGridBuilder\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from pathlib import Path\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType, BooleanType, ArrayType\n",
    "from pyspark.sql.functions import rand, split, posexplode, explode, col, collect_list, struct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "data_dir = 'recsys19/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the spark session used for all calculations\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('recsys19') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata file\n",
    "# Construct the schema so the datatypes are set correctly.\n",
    "# First the file is read without a schema to get all column names\n",
    "df_meta = spark.read.csv(\n",
    "    data_dir + 'df_metadata.csv',\n",
    "    header=True,\n",
    ")\n",
    "\n",
    "# Actually construct the schema\n",
    "schema_meta = StructType() \\\n",
    "    .add('pandas_id', IntegerType()) \\\n",
    "    .add('item_id', StringType())\n",
    "\n",
    "\n",
    "for subtype in df_meta.schema:\n",
    "    name = subtype.name\n",
    "    \n",
    "    if name in ('_c0', 'item_id'):\n",
    "        continue\n",
    "        \n",
    "    schema_meta = schema_meta.add(\n",
    "        StructField(name, IntegerType()))\n",
    "\n",
    "# Read again, this time with schema\n",
    "df_meta = spark.read.csv(\n",
    "    data_dir + 'df_metadata.csv',\n",
    "    header=True,\n",
    "    schema=schema_meta\n",
    ")\n",
    "\n",
    "# This column was added inadvertently during preprocessing, drop it\n",
    "df_meta = df_meta.drop('pandas_id')\n",
    "\n",
    "df_meta.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sessions file\n",
    "# Construct the schema so the datatypes are set correctly.\n",
    "schema_sessions = StructType([\n",
    "    StructField(\"pandas_id\", IntegerType()),\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"session_id\", StringType()),\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"step\", IntegerType()),\n",
    "    StructField(\"action_type\", StringType()),\n",
    "    StructField(\"reference\", StringType()),\n",
    "    StructField(\"platform\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"device\", StringType()),\n",
    "    StructField(\"current_filters\", StringType()),\n",
    "    StructField(\"impressions\", StringType()),\n",
    "    StructField(\"prices\", StringType()),\n",
    "    StructField(\"is_validation\", BooleanType()),\n",
    "    StructField(\"is_train\", BooleanType()),\n",
    "])\n",
    "\n",
    "df_sessions = spark.read.csv(\n",
    "#     data_dir + 'df_sessions_small.csv',\n",
    "    data_dir + 'df_sessions_full.csv',\n",
    "    header=True,\n",
    "    schema=schema_sessions\n",
    ")\n",
    "\n",
    "# This column was added inadvertently during preprocessing, drop it\n",
    "df_sessions = df_sessions.drop('pandas_id')\n",
    "\n",
    "df_sessions.first()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract as much information as possible from each session.\n",
    "# We'll call the per-action data facets\n",
    "\n",
    "def add_metadata(df):\n",
    "    df = df.selectExpr('session_id', 'reference AS item_id')\n",
    "    \n",
    "    df = df.join(\n",
    "        df_meta,\n",
    "        'item_id'\n",
    "    )\n",
    "    \n",
    "    df = df.drop('item_id')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# For each facet, define a function called `preprocess_<facet-name>`\n",
    "# This function is passed the rows with matching action types and should\n",
    "# output the facet data. This data hast to have a `session_id` so it can\n",
    "# be joined with the other data.\n",
    "preprocess_interact_picture = add_metadata\n",
    "preprocess_clickout = add_metadata\n",
    "preprocess_interact_rating = add_metadata\n",
    "preprocess_interact_info = add_metadata\n",
    "preprocess_interact_deals = add_metadata\n",
    "\n",
    "    \n",
    "# Metadata about sessions:\n",
    "#   action_name : (name_as_in_dataset, short_name)\n",
    "facets = {\n",
    "    'interact_picture': ('interaction item image', 'img'),\n",
    "    'clickout': ('clickout item', 'cout'),\n",
    "    # '': ('search for destination',),\n",
    "    # 'search_item': ('search for item',),\n",
    "    'interact_rating': ('interaction item rating', 'rat'),\n",
    "    # '': ('search for poi',),\n",
    "    'interact_info': ('interaction item info', 'info'),\n",
    "    'interact_deals': ('interaction item deals', 'deal'),\n",
    "    # '': ('change of sort order',),\n",
    "    # 'filter': ('filter selection',),\n",
    "}\n",
    "\n",
    "\n",
    "# Extract the facets\n",
    "facet_data = {}\n",
    "for name, params in facets.items():\n",
    "    action_name, prefix = params\n",
    "    \n",
    "    print(f'Computing facet {name} (\"{prefix}\")')\n",
    "\n",
    "    # Select the rows applicable to this facet\n",
    "    facet_in = df_sessions.where(f\"action_type == '{action_name}'\")\n",
    "\n",
    "    # Apply the facet's preprocessing function\n",
    "    preproc_function = globals()[f'preprocess_{name}']\n",
    "    facet_out = preproc_function(facet_in)\n",
    "    \n",
    "    facet_data[prefix] = facet_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the facets & multiplex the impressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that all data has been extracted, merge it into a combined\n",
    "# dataset holding all the information.\n",
    "all_cols      = df_sessions.filter('action_type = \"clickout item\"').cache()\n",
    "data_cols     = all_cols.select('session_id', 'platform', 'city', 'device', 'is_validation', 'is_train', 'reference')\n",
    "distinct_cols = all_cols.select('session_id', 'impressions').distinct()\n",
    "\n",
    "# Add columns from the raw input. They were removed before so the 'distinct' operation doesn't operate on them\n",
    "df_joined = distinct_cols.join(\n",
    "    data_cols,\n",
    "    'session_id'\n",
    ")\n",
    "\n",
    "# Add columns supplied by the facets\n",
    "for prefix, subdata in facet_data.items():\n",
    "    print(f'Adding {prefix} facet')\n",
    "    \n",
    "    # Add a prefix to the column names of the subdata\n",
    "    subdata = subdata.toDF(*((f'fac_{prefix}_{c}' if c != 'session_id' else c) for c in subdata.columns ))\n",
    "    \n",
    "    # Join the dataframes\n",
    "    df_joined = df_joined.join(\n",
    "        subdata,\n",
    "        on='session_id',\n",
    "        how='left_outer'\n",
    "    )\n",
    "\n",
    "\n",
    "# Per-impression processing\n",
    "\n",
    "# Explode the impressions\n",
    "df_joined = df_joined.withColumn(\n",
    "    'impressions',\n",
    "    split(col('impressions'), '\\\\|')\n",
    ")\n",
    "\n",
    "df_joined = df_joined.selectExpr(\n",
    "    '*',\n",
    "    'posexplode(impressions) as (impression_index, impression_id)',\n",
    ").drop('impressions')\n",
    "\n",
    "\n",
    "# Add the ground truth\n",
    "df_joined = df_joined.withColumn(\n",
    "    'gt',\n",
    "    (df_joined.impression_id == df_joined.reference).cast('float')\n",
    ").drop('reference')\n",
    "\n",
    "\n",
    "# Add the per-impression metadata\n",
    "prefixed_df_meta = df_meta.toDF(*((f'gt_{c}' if c != 'item_id' else c) for c in df_meta.columns ))\n",
    "\n",
    "\n",
    "df_joined = df_joined.join(\n",
    "    prefixed_df_meta,\n",
    "    on=df_joined.impression_id == prefixed_df_meta.item_id\n",
    "    how='left_outer'\n",
    ").drop('item_id')\n",
    "\n",
    "\n",
    "# Names of all columns that are used for training the ML model\n",
    "feature_col_names = [cn for cn in df_joined.columns if cn.startswith('gt_') or cn.startswith('fac_')]\n",
    "print('Feature columns: ', feature_col_names)\n",
    "\n",
    "# Merge the ML features into a single vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_col_names,\n",
    "    outputCol=\"ml_features\",\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "\n",
    "df_joined = assembler.transform(df_joined)\n",
    "\n",
    "df_joined.cache()\n",
    "\n",
    "# df_joined.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some statistics\n",
    "print(f'sessions: {distinct_cols.count()}')\n",
    "print(f'df_joined: {df_joined.count()} rows, {len(df_joined.columns)} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the estimator\n",
    "train_rows = df_joined.filter(df_joined.gt.isNotNull())\n",
    "\n",
    "estimator = LinearRegression(featuresCol='ml_features', labelCol='gt', maxIter=10)\n",
    "estimator_model = estimator.fit(train_rows)\n",
    "\n",
    "print(estimator_model.intercept, estimator_model.coefficients[:5], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the predictions DF\n",
    "# Make predictions\n",
    "df_preds = df_joined.filter(df_joined.gt.isNull())\n",
    "\n",
    "df_preds = estimator_model.transform(df_preds)\n",
    "\n",
    "# Gather the impression rows\n",
    "df_preds = df_preds.withColumn(\n",
    "    'pairs',\n",
    "    struct(['impression_id', 'prediction'])\n",
    ")\n",
    "    \n",
    "df_preds = df_preds.groupBy('session_id').agg(collect_list('pairs').alias('collected'))\n",
    "\n",
    "# Postprocess with pandas\n",
    "# Bring the dataframe into the format needed by the ensemble\n",
    "df_preds = df_preds.toPandas()\n",
    "\n",
    "df_preds['item_recommendations'] = df_preds.collected.apply(lambda x: ' '.join(str(y[0]) for y in x))\n",
    "df_preds['item_probs'] = df_preds.collected.apply(lambda x: ' '.join(str(y[1]) for y in x))\n",
    "\n",
    "df_preds.drop(columns=['collected'], inplace=True)\n",
    "\n",
    "df_preds.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all columns required by the output format\n",
    "df_results = df_sessions.select(\n",
    "    'user_id',\n",
    "    'session_id',\n",
    "    'timestamp',\n",
    "    'step',\n",
    "   ).filter(df_sessions.reference.isNull()) \\\n",
    "    .filter('action_type = \"clickout item\"') \\\n",
    "    .distinct() \\\n",
    "    .toPandas()\n",
    "    \n",
    "print(len(df_results))\n",
    "\n",
    "df_results = df_results.merge(\n",
    "    df_preds,\n",
    "    on='session_id'\n",
    ")\n",
    "\n",
    "df_results['timestamp'] = pd.to_datetime(df_results.timestamp).astype(np.int64) // 10 ** 9\n",
    "\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the results\n",
    "df_results.to_csv('session_based_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
