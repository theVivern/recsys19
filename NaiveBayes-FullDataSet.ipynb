{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import time\n",
    "\n",
    "# %% Imports\n",
    "from pathlib import Path\n",
    "import sys\n",
    "root_dir = Path().resolve()\n",
    "sys.path.append(str(root_dir / 'src'))\n",
    "\n",
    "from recsys_common import *\n",
    "from recsys_naive_bayes_processing import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config= {\n",
    "    'save_train_test_val': True,\n",
    "    'load_fitted_model': False,\n",
    "    \n",
    "    'use_subset': True,\n",
    "    'subset_frac': 0.05,\n",
    "    'use_validation': True,\n",
    "    'validation_frac': 0.25,\n",
    "    'reference_to_nan_frac': 1,\n",
    "    'reference_to_nan_seed': 1234,\n",
    "    \n",
    "    'session_length': 1,\n",
    "    'drop_no_references': True,\n",
    "    \n",
    "    'train_session_chunksize': 5000,\n",
    "    'parts_nrows_test': 5000,\n",
    "    'parts_path_to_data': root_dir / 'cache' / 'parts',\n",
    "    'data_path': root_dir / 'cache'\n",
    "    }\n",
    "\n",
    "\n",
    "if not config['use_subset']:\n",
    "    config['subset_frac']=1\n",
    "\n",
    "config['le_pickle_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) + '_le.pickle')\n",
    "config['train_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_train.csv')\n",
    "config['train_last_step_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_train_last_step.csv')\n",
    "config['test_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_test.csv')\n",
    "config['val_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_val.csv')\n",
    "config['model_pickle_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_model.pickle')\n",
    "config['val_long_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_val_long.csv')\n",
    "config['output_recsys_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_output_recsys.csv')\n",
    "config['output_meta_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_output_meta.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta=get_metadata()\n",
    "# meta.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['item_id']=meta['item_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sessions\n",
      "Filter session with no clickout\n",
      "user_id                         object\n",
      "session_id                      object\n",
      "timestamp          datetime64[ns, UTC]\n",
      "step                             int64\n",
      "action_type                     object\n",
      "reference                       object\n",
      "platform                        object\n",
      "city                            object\n",
      "device                          object\n",
      "current_filters                 object\n",
      "impressions                     object\n",
      "prices                          object\n",
      "is_validation                     bool\n",
      "is_train                          bool\n",
      "target                          object\n",
      "dtype: object\n",
      "Quick unit test\n",
      "893499\n",
      "893499\n",
      "10336\n",
      "0\n",
      "0\n",
      "19995\n",
      "10336\n",
      "59661\n",
      "0\n",
      "26345\n",
      "12856\n",
      "Train encoders and save\n",
      "['change of sort order' 'clickout item' 'filter selection'\n",
      " 'interaction item deals' 'interaction item image' 'interaction item info'\n",
      " 'interaction item rating' 'search for destination' 'search for item'\n",
      " 'search for poi']\n",
      "['AA' 'AE' 'AR' 'AT' 'AU' 'BE' 'BG' 'BR' 'CA' 'CH' 'CL' 'CN' 'CO' 'CZ'\n",
      " 'DE' 'DK' 'EC' 'ES' 'FI' 'FR' 'GR' 'HK' 'HR' 'HU' 'ID' 'IE' 'IL' 'IN'\n",
      " 'IT' 'JP' 'KR' 'MX' 'MY' 'NL' 'NO' 'NZ' 'PE' 'PH' 'PL' 'PT' 'RO' 'RS'\n",
      " 'RU' 'SE' 'SG' 'SI' 'SK' 'TH' 'TR' 'TW' 'UK' 'US' 'UY' 'VN' 'ZA']\n",
      "['A Teixeira, Spain' 'Aachen, Germany' 'Aadorf, Switzerland' ...\n",
      " 'Żarki, Poland' 'Žabljak, Montenegro' 'Žilina, Slovakia']\n",
      "['desktop' 'mobile' 'tablet']\n",
      "Get Splits\n",
      "train (537963, 10)\n",
      "test (200069, 13)\n",
      "val (155467, 13)\n",
      "Save either test or val\n",
      "delete session, test and val\n",
      "save train\n",
      "delete train\n"
     ]
    }
   ],
   "source": [
    "if config['save_train_test_val']:\n",
    "    print('Getting sessions')\n",
    "    sessions=get_sessions(config['use_subset'],\n",
    "                          config['subset_frac'],\n",
    "                          config['use_validation'],\n",
    "                          config['validation_frac'],\n",
    "                          config['reference_to_nan_frac'],\n",
    "                          config['reference_to_nan_seed'])\n",
    "\n",
    "    print('Filter session with no clickout')\n",
    "    if (not config['use_validation']) & (not config['use_subset']):\n",
    "        print('filtering sessions with clickout')\n",
    "        sessions=filter_sessions_with_no_clicks(sessions)\n",
    "\n",
    "#     print('Split impressions and prices')\n",
    "#     sessions['impressions']=sessions['impressions'].str.split('\\\\|')\n",
    "\n",
    "#     sessions['prices']=sessions['prices'].str.split('\\\\|')\n",
    "\n",
    "    print(sessions.dtypes)\n",
    "    # sessions.head()\n",
    "\n",
    "    print('Quick unit test')\n",
    "\n",
    "    print(len(sessions.index))\n",
    "    print(len(sessions.index.unique()))\n",
    "\n",
    "    if not config['use_validation']:\n",
    "        sessions['is_validation']=False\n",
    "        sessions['target']=np.NaN\n",
    "\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==True),'target'].count())\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False),'target'].count())\n",
    "    print(sessions.loc[(sessions.is_train==False) & (sessions.is_validation==False),'target'].count())\n",
    "\n",
    "    print(sessions.loc[(sessions.is_validation==True) & (sessions.action_type=='clickout item'),'step'].count())\n",
    "    print(sessions.loc[(sessions.is_validation==True) & (sessions.action_type=='clickout item') & (sessions.reference.isnull()),'step'].count())\n",
    "\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False) & (sessions.action_type=='clickout item') ,'step'].count())\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False) & (sessions.action_type=='clickout item') & (sessions.reference.isnull()),'step'].count())\n",
    "\n",
    "    print(sessions.loc[(sessions.is_train==False) & (sessions.is_validation==False) & (sessions.action_type=='clickout item'),'step'].count())\n",
    "    print(sessions.loc[(sessions.is_train==False) & (sessions.is_validation==False) & (sessions.action_type=='clickout item') & (sessions.reference.isnull()),'step'].count())\n",
    "\n",
    "    print('Train encoders and save')\n",
    "\n",
    "    columns_to_encode = ['action_type','platform','city','device']\n",
    "\n",
    "    encoders = {}\n",
    "    for col in columns_to_encode:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        encoders[col]=le.fit(sessions[col])\n",
    "        print(encoders[col].classes_)\n",
    "    #     val_wide[col]=encoders[col].transform(val_wide[col])\n",
    "\n",
    "    with open(config['le_pickle_path'], 'wb') as handle:\n",
    "        pickle.dump(encoders, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # with open(config['le_pickle_path'], 'rb') as handle:\n",
    "    #     b = pickle.load(handle)\n",
    "\n",
    "    print('Get Splits')\n",
    "\n",
    "    train = sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False)] \\\n",
    "                        .drop(['impressions','prices','is_train','is_validation','target'],axis=1) \\\n",
    "                        .reset_index(drop=True)\n",
    "\n",
    "    test = sessions.loc[sessions.is_train==False] \\\n",
    "                       .drop(['is_train','is_validation'],axis=1) \\\n",
    "                       .reset_index(drop=True)\n",
    "\n",
    "    val = sessions.loc[(sessions.is_train==True) & (sessions.is_validation==True)] \\\n",
    "                       .drop(['is_train','is_validation'],axis=1) \\\n",
    "                       .reset_index(drop=True)\n",
    "\n",
    "    print('train',train.shape)\n",
    "    print('test',test.shape)\n",
    "    print('val',val.shape)\n",
    "\n",
    "\n",
    "    print('Save either test or val')\n",
    "    if config['use_validation']:\n",
    "        val.to_csv(config['val_csv_path'])\n",
    "    else:\n",
    "        test.to_csv(config['test_csv_path'])\n",
    "\n",
    "\n",
    "    print('delete session, test and val')\n",
    "    try:\n",
    "        del sessions, test, val\n",
    "    except NameError:\n",
    "        pass\n",
    "    else:\n",
    "        gc.collect()\n",
    "     \n",
    "    \n",
    "\n",
    "\n",
    "    print('save train')\n",
    "    train.to_csv(config['train_csv_path'])\n",
    "\n",
    "else:\n",
    "    print('loading train and encoders...')\n",
    "    \n",
    "    train=pd.read_csv(config['train_csv_path'])\n",
    "    train.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "    \n",
    "    with open(config['le_pickle_path'], 'rb') as handle:\n",
    "        encoders = pickle.load(handle)\n",
    "\n",
    "    print('done')\n",
    "    \n",
    "\n",
    "last_step_per_session=train.groupby('session_id',sort=False)['step'].max().reset_index().to_csv(config['train_last_step_csv_path'])\n",
    "\n",
    "names=train.columns\n",
    "\n",
    "# classes=list(set(train.loc[(train.action_type=='clickout item') & (train.step>1),'reference']))\n",
    "classes=list(set(train.loc[(train.action_type=='clickout item'),'reference']))\n",
    "\n",
    "print('delete train')\n",
    "try:\n",
    "    del train\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference|1\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop=([])\n",
    "for i in range(config['session_length']):\n",
    "    print('reference|' + str(i+1))\n",
    "    cols_to_drop=np.append(cols_to_drop,'reference|' + str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6386, 163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NewOffice3\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6598, 163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NewOffice3\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6760, 163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NewOffice3\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6956, 163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NewOffice3\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6737, 163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NewOffice3\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6541, 163)\n"
     ]
    }
   ],
   "source": [
    "if not config['load_fitted_model']:\n",
    "    start = time.time()\n",
    "    reader=pd.read_csv(config['train_last_step_csv_path'], chunksize=config['train_session_chunksize'])\n",
    "\n",
    "    skiprows=0\n",
    "    clf=BernoulliNB()\n",
    "    for i,chunk in enumerate(reader):\n",
    "        nrows=chunk.step.sum()\n",
    "        train_part=pd.read_csv(config['train_csv_path'],header=0,skiprows=skiprows,nrows=nrows,names=names)\n",
    "\n",
    "        train_wide = process_train_naives_bayes(data=train_part, metadata=meta, session_length=config['session_length'], encode = True,encoders=encoders,cols_to_encode=list(encoders.keys()))\n",
    "\n",
    "        del train_part\n",
    "\n",
    "        if config['drop_no_references']:\n",
    "            train_wide_not_allnull=train_wide[(train_wide.iloc[:,0:(2*config['session_length']-1)].T != 0).all()].copy()\n",
    "        else:\n",
    "            train_wide_not_allnull=train_wide.copy()\n",
    "\n",
    "\n",
    "        del train_wide\n",
    "\n",
    "        print(train_wide_not_allnull.shape)\n",
    "        \n",
    "        if not train_wide_not_allnull.empty:\n",
    "            if i==0:\n",
    "                clf.partial_fit(train_wide_not_allnull \\\n",
    "                            .drop(np.append(cols_to_drop,'y'),axis=1), \\\n",
    "                                   np.ravel(train_wide_not_allnull.y), \\\n",
    "                                   classes = classes)\n",
    "            else:\n",
    "                clf.partial_fit(train_wide_not_allnull \\\n",
    "                            .drop(np.append(cols_to_drop,'y'),axis=1), \\\n",
    "                                   np.ravel(train_wide_not_allnull.y))\n",
    "\n",
    "        skiprows += nrows\n",
    "\n",
    "    stop = time.time()\n",
    "    print('Seconds elapsed  :', stop-start) \n",
    "    \n",
    "    # Output a pickle file for the model\n",
    "    joblib.dump(clf, config['model_pickle_path'])\n",
    "    \n",
    "    del train_wide_not_allnull\n",
    "    \n",
    "else:\n",
    "    clf=joblib.load(config['model_pickle_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get test data in wide format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split for partial fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['use_validation']:\n",
    "    data=pd.read_csv(config['val_csv_path'],parse_dates=['timestamp'])\n",
    "else:\n",
    "    data=pd.read_csv(config['test_csv_path'],parse_dates=['timestamp'])\n",
    "\n",
    "data.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "val_wide = process_test_naives_bayes(data=data, metadata=meta, session_length=config['session_length'], encode = True,encoders=encoders,cols_to_encode=list(encoders.keys()))\n",
    "val_wide=val_wide.loc[val_wide.impressions!=0]\n",
    "\n",
    "\n",
    "try:\n",
    "    del test\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    gc.collect()\n",
    "\n",
    "if config['drop_no_references']:\n",
    "    val_wide_not_allnull=val_wide.loc[(val_wide.iloc[:,0:(2*config['session_length']-1)].T != 0).all()].copy()\n",
    "else:\n",
    "    val_wide_not_allnull=val_wide.copy()\n",
    "\n",
    "val_wide_not_allnull['impressions']=val_wide_not_allnull['impressions'].str.split('\\\\|')\n",
    "\n",
    "try:\n",
    "    del val_wide\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_wide_not_allnull.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create long table for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imp=val_wide_not_allnull[['impressions','target']].reset_index().copy()\n",
    "\n",
    "val_imp_long=val_imp.impressions.apply(pd.Series) \\\n",
    "    .merge(val_imp, right_index = True, left_index = True) \\\n",
    "    .drop([\"impressions\"], axis = 1)  \\\n",
    "    .melt(id_vars = ['key','target'], value_name = \"impressions\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna() \\\n",
    "    .sort_values('key') \\\n",
    "    .reset_index(drop=True) \\\n",
    "    .copy()\n",
    "\n",
    "try:\n",
    "    del val_imp\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get row, col indexes for extracting probabilities from pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index=val_imp_long['key'].copy()\n",
    "col_index=val_imp_long['impressions'].copy()\n",
    "\n",
    "mask_in_class = col_index.isin(clf.classes_) #True is in class\n",
    "col_index.loc[~mask_in_class]='not_in_class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do predict and munge output on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_wide_not_allnull.shape)\n",
    "\n",
    "n_splits = round(val_wide_not_allnull.shape[0]/config['parts_nrows_test'])\n",
    "index_split = round(val_wide_not_allnull.shape[0]/n_splits)\n",
    "\n",
    "impression_probs=([])\n",
    "for i in range(n_splits): # n_splits\n",
    "    if (i+1)==n_splits:\n",
    "        pred=clf.predict_proba(val_wide_not_allnull.iloc[(index_split*(i)):,:]. \\\n",
    "                                         drop(np.append(cols_to_drop,['timestamp', 'impressions','target']),axis=1))\n",
    "        \n",
    "        preddf=pd.DataFrame(data=pred,columns=clf.classes_,index=val_wide_not_allnull.iloc[(index_split*(i)):,:].index)\n",
    "        preddf['not_in_class']=0 # need to return 0 in case impression not in clf.classes_\n",
    "        \n",
    "        first_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):,:].head(1).index\n",
    "        first_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(first_key_last_pred)].index.min()\n",
    "\n",
    "        last_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):,:].tail(1).index\n",
    "        last_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(last_key_last_pred)].index.max()\n",
    "\n",
    "        row_index[first_index_for_row_col:(last_index_for_row_col+1)]\n",
    "        \n",
    "        impression_probs=np.append(impression_probs,preddf.lookup(row_index[first_index_for_row_col:(last_index_for_row_col+1)], \\\n",
    "                                              col_index[first_index_for_row_col:(last_index_for_row_col+1)]),axis=0)\n",
    "        \n",
    "    else:\n",
    "        pred=clf.predict_proba(val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:]. \\\n",
    "                                         drop(np.append(cols_to_drop,['timestamp', 'impressions','target']),axis=1))\n",
    "        \n",
    "        \n",
    "        preddf=pd.DataFrame(data=pred,columns=clf.classes_,index=val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:].index)\n",
    "        preddf['not_in_class']=0 # need to return 0 in case impression not in clf.classes_\n",
    "        \n",
    "        first_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:].head(1).index\n",
    "        first_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(first_key_last_pred)].index.min()\n",
    "\n",
    "        last_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:].tail(1).index\n",
    "        last_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(last_key_last_pred)].index.max()\n",
    "\n",
    "        row_index[first_index_for_row_col:(last_index_for_row_col+1)]\n",
    "        \n",
    "        impression_probs=np.append(impression_probs,preddf.lookup(row_index[first_index_for_row_col:(last_index_for_row_col+1)], \\\n",
    "                                              col_index[first_index_for_row_col:(last_index_for_row_col+1)]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Munge to get rank and RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imp_long['probs'] = impression_probs\n",
    "\n",
    "val_imp_long.sort_values(['key','probs'],ascending=[True,False],inplace=True)\n",
    "\n",
    "val_imp_long['rank'] = 1\n",
    "val_imp_long['rank'] = val_imp_long.groupby('key')['rank'].cumsum()\n",
    "\n",
    "val_imp_long['RR']=1/val_imp_long['rank']\n",
    "\n",
    "val_imp_long.to_csv(config['val_long_csv_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_imp_wide=val_imp_long.pivot(index='key',columns='rank',values=['impressions','probs']).copy()\n",
    "\n",
    "# collapse column multi index for ease of indexing\n",
    "rank_imp_wide.columns=rank_imp_wide.columns.map(lambda x: '|'.join([str(i) for i in x]))\n",
    "    \n",
    "rank_imp_wide=rank_imp_wide.join(val_wide_not_allnull['timestamp'], on='key')\n",
    "\n",
    "rank_imp_wide['timestamp']=rank_imp_wide['timestamp'].astype(np.int64)//10**9\n",
    "\n",
    "rank_imp_wide.reset_index(inplace=True)\n",
    "\n",
    "rank_imp_wide[['user_id','session_id','step']]=rank_imp_wide['key'].str.split('_',expand=True)\n",
    "\n",
    "rank_imp_wide.drop(['key'],axis=1,inplace=True)\n",
    "\n",
    "rank_imp_wide['item_recommendations'] = rank_imp_wide.iloc[:,:25].apply(\n",
    "    lambda x: ' '.join(x.dropna()),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "rank_imp_wide.drop(rank_imp_wide.columns[:25],axis=1,inplace=True)\n",
    "\n",
    "rank_imp_wide['item_probs'] = rank_imp_wide.iloc[:,:25].apply(\n",
    "    lambda x: ' '.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "rank_imp_wide.drop(rank_imp_wide.columns[:25],axis=1,inplace=True)\n",
    "\n",
    "cols=rank_imp_wide.columns.tolist()\n",
    "cols=cols[1:3]+cols[:1]+cols[3:4]+cols[-2:]\n",
    "rank_imp_wide=rank_imp_wide[cols]\n",
    "\n",
    "rank_imp_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_imp_wide.drop('item_probs',axis=1).to_csv(config['output_recsys_csv_path'],index=False)\n",
    "rank_imp_wide.to_csv(config['output_meta_csv_path'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['use_validation']:\n",
    "    MRR_NB=val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)) \\\n",
    "                           & (val_imp_long.probs!=0.0),'RR'].mean()\n",
    "    MRR_parse_imp=val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)),'RR'].mean()\n",
    "    \n",
    "    print(MRR_NB)\n",
    "    print(MRR_parse_imp)\n",
    "    print('support MRR_NB: ', str(val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)) & (val_imp_long.probs!=0.0),'RR'].count()))\n",
    "    print('support MRR_NB: ', str(val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)),'RR'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)) & (val_imp_long.probs!=0.0),'RR'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add lost sessions back"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
