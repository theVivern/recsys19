{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NewOffice3\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import time\n",
    "\n",
    "# %% Imports\n",
    "from pathlib import Path\n",
    "import sys\n",
    "root_dir = Path().resolve()\n",
    "sys.path.append(str(root_dir / 'src'))\n",
    "\n",
    "from recsys_common import *\n",
    "from recsys_naive_bayes_processing import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config= {\n",
    "    'save_train_test_val': True,\n",
    "    'load_fitted_model': False,\n",
    "    \n",
    "    'use_subset': True,\n",
    "    'subset_frac': 0.05,\n",
    "    'use_validation': True,\n",
    "    'validation_frac': 0.25,\n",
    "    'reference_to_nan_frac': 1,\n",
    "    'reference_to_nan_seed': 1234,\n",
    "    \n",
    "    'session_length': 1,\n",
    "    'drop_no_references': True,\n",
    "    'train_model_on_test_data': True,\n",
    "    'add_prices': False,\n",
    "    'add_hour': False,\n",
    "    'cols_to_append': [],#['platform','city','device'],\n",
    "    'drop_action_type': True,\n",
    "    \n",
    "    'train_session_chunksize': 10000,\n",
    "    'parts_nrows_test': 10000,\n",
    "    'data_path': root_dir / 'cache'\n",
    "    }\n",
    "\n",
    "\n",
    "if not config['use_subset']:\n",
    "    config['subset_frac']=1\n",
    "\n",
    "root_path=('NB_data_sub_' + str(int(100*config['subset_frac'])).zfill(3) \\\n",
    " + '_sl_' + str(config['session_length']) \\\n",
    " + '_val_' + str(int(config['use_validation'])))                       \n",
    "                       \n",
    "config['le_pickle_path']=config['data_path'] / (root_path + '_le.pickle')\n",
    "\n",
    "config['train_wide_pickle_path']=config['data_path'] / (root_path +  '_train_wide.pickle')\n",
    "config['classes_pickle_path']=config['data_path'] / (root_path +  '_classes.pickle')\n",
    "\n",
    "config['test_pickle_path']=config['data_path'] / (root_path +  '_test.pickle')\n",
    "config['val_pickle_path']=config['data_path'] / (root_path +  '_val.pickle')\n",
    "\n",
    "config['prices_pickle_path']=config['data_path'] / 'mean_prices.pickle'\n",
    "\n",
    "config['model_pickle_path']=config['data_path'] / (root_path +  '_model.pickle')\n",
    "config['val_long_csv_path']=config['data_path'] / (root_path +  '_val_long.csv')\n",
    "\n",
    "config['output_recsys_csv_path']=config['data_path'] / (root_path +  '_output_recsys.csv')\n",
    "config['output_meta_csv_path']=config['data_path'] / (root_path +  '_output_meta.csv')\n",
    "config['output_meta_only_fiited_csv_path']=config['data_path'] / (root_path +  '_output_meta_only_fiited.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta=get_metadata()\n",
    "# meta.dtypes\n",
    "\n",
    "meta['item_id']=meta['item_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sessions\n",
      "Drop session with no clickout\n",
      "user_id                         object\n",
      "session_id                      object\n",
      "timestamp          datetime64[ns, UTC]\n",
      "step                             int64\n",
      "action_type                     object\n",
      "reference                       object\n",
      "platform                        object\n",
      "city                            object\n",
      "device                          object\n",
      "current_filters                 object\n",
      "impressions                     object\n",
      "prices                          object\n",
      "is_validation                     bool\n",
      "is_train                          bool\n",
      "target                          object\n",
      "dtype: object\n",
      "Quick unit test\n",
      "893499\n",
      "893499\n",
      "10336\n",
      "0\n",
      "0\n",
      "19995\n",
      "10336\n",
      "59661\n",
      "0\n",
      "26345\n",
      "12856\n",
      "Get possible classes\n",
      "Train encoders and save\n",
      "['change of sort order' 'clickout item' 'filter selection'\n",
      " 'interaction item deals' 'interaction item image' 'interaction item info'\n",
      " 'interaction item rating' 'search for destination' 'search for item'\n",
      " 'search for poi']\n",
      "['AA' 'AE' 'AR' 'AT' 'AU' 'BE' 'BG' 'BR' 'CA' 'CH' 'CL' 'CN' 'CO' 'CZ'\n",
      " 'DE' 'DK' 'EC' 'ES' 'FI' 'FR' 'GR' 'HK' 'HR' 'HU' 'ID' 'IE' 'IL' 'IN'\n",
      " 'IT' 'JP' 'KR' 'MX' 'MY' 'NL' 'NO' 'NZ' 'PE' 'PH' 'PL' 'PT' 'RO' 'RS'\n",
      " 'RU' 'SE' 'SG' 'SI' 'SK' 'TH' 'TR' 'TW' 'UK' 'US' 'UY' 'VN' 'ZA']\n",
      "['A Teixeira, Spain' 'Aachen, Germany' 'Aadorf, Switzerland' ...\n",
      " 'Żarki, Poland' 'Žabljak, Montenegro' 'Žilina, Slovakia']\n",
      "['desktop' 'mobile' 'tablet']\n",
      "Get Splits\n",
      "train (537963, 10)\n",
      "test (200069, 13)\n",
      "val (155467, 13)\n",
      "train user_id                         object\n",
      "session_id                      object\n",
      "timestamp          datetime64[ns, UTC]\n",
      "step                             int64\n",
      "action_type                     object\n",
      "reference                       object\n",
      "platform                        object\n",
      "city                            object\n",
      "device                          object\n",
      "current_filters                 object\n",
      "dtype: object\n",
      "test user_id                         object\n",
      "session_id                      object\n",
      "timestamp          datetime64[ns, UTC]\n",
      "step                             int64\n",
      "action_type                     object\n",
      "reference                       object\n",
      "platform                        object\n",
      "city                            object\n",
      "device                          object\n",
      "current_filters                 object\n",
      "impressions                     object\n",
      "prices                          object\n",
      "target                          object\n",
      "dtype: object\n",
      "val user_id                         object\n",
      "session_id                      object\n",
      "timestamp          datetime64[ns, UTC]\n",
      "step                             int64\n",
      "action_type                     object\n",
      "reference                       object\n",
      "platform                        object\n",
      "city                            object\n",
      "device                          object\n",
      "current_filters                 object\n",
      "impressions                     object\n",
      "prices                          object\n",
      "target                          object\n",
      "dtype: object\n",
      "Save either test or val\n",
      "delete session, test and val\n",
      "Calc. train_wide\n",
      "delete train\n",
      "save train_wide and classes\n"
     ]
    }
   ],
   "source": [
    "if config['save_train_test_val']:\n",
    "    print('Getting sessions')\n",
    "    sessions=get_sessions(config['use_subset'],\n",
    "                          config['subset_frac'],\n",
    "                          config['use_validation'],\n",
    "                          config['validation_frac'],\n",
    "                          config['reference_to_nan_frac'],\n",
    "                          config['reference_to_nan_seed'])\n",
    "\n",
    "    print('Drop session with no clickout')\n",
    "    if (not config['use_validation']) & (not config['use_subset']):\n",
    "        sessions=filter_sessions_with_no_clicks(sessions)\n",
    "\n",
    "        \n",
    "    columns_to_encode = ['action_type','platform','city','device']\n",
    "\n",
    "    if config['add_prices']:\n",
    "        print('adding prices')\n",
    "        prices=pd.read_pickle(config['prices_pickle_path'])\n",
    "        sessions=sessions.join(prices.set_index('reference'),on='reference')\n",
    "#         sessions['city_price']=sessions['city'] + sessions['mean_prices'].astype(str)\n",
    "#         columns_to_encode=np.append(columns_to_encode,['city_price'])\n",
    "    \n",
    "    if config['add_hour']:\n",
    "        sessions['hour']=sessions.timestamp.dt.hour.astype(str)\n",
    "        sessions['city_hour']=sessions['city'] + sessions['hour']\n",
    "        sessions.drop('hour',axis=1,inplace=True)\n",
    "        columns_to_encode=np.append(columns_to_encode,['city_hour'])\n",
    "        config['cols_to_append']=np.append(config['cols_to_append'],['city_hour'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(sessions.dtypes)\n",
    "    # sessions.head()\n",
    "\n",
    "    print('Quick unit test')\n",
    "\n",
    "    print(len(sessions.index))\n",
    "    print(len(sessions.index.unique()))\n",
    "\n",
    "    if not config['use_validation']:\n",
    "        sessions['is_validation']=False\n",
    "        sessions['target']=np.NaN\n",
    "\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==True),'target'].count())\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False),'target'].count())\n",
    "    print(sessions.loc[(sessions.is_train==False) & (sessions.is_validation==False),'target'].count())\n",
    "\n",
    "    print(sessions.loc[(sessions.is_validation==True) & (sessions.action_type=='clickout item'),'step'].count())\n",
    "    print(sessions.loc[(sessions.is_validation==True) & (sessions.action_type=='clickout item') & (sessions.reference.isnull()),'step'].count())\n",
    "\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False) & (sessions.action_type=='clickout item') ,'step'].count())\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False) & (sessions.action_type=='clickout item') & (sessions.reference.isnull()),'step'].count())\n",
    "\n",
    "    print(sessions.loc[(sessions.is_train==False) & (sessions.is_validation==False) & (sessions.action_type=='clickout item'),'step'].count())\n",
    "    print(sessions.loc[(sessions.is_train==False) & (sessions.is_validation==False) & (sessions.action_type=='clickout item') & (sessions.reference.isnull()),'step'].count())\n",
    "    \n",
    "    print('Get possible classes')\n",
    "#     classes=list(set(sessions.loc[(sessions.action_type=='clickout item') & ~(sessions.reference.isnull()),'reference']))\n",
    "    classes=list(set(sessions.loc[(sessions.action_type=='clickout item') & ~(sessions.reference.isnull()) & (sessions.step>1),'reference']))\n",
    "    \n",
    "#     def get_classes(sessions):\n",
    "#         refs=sessions.loc[(sessions.action_type=='clickout item') & ~(sessions.reference.isnull()),'reference'].tolist()\n",
    "\n",
    "#         sessions['impressions']=sessions['impressions'].str.split('\\\\|')\n",
    "#         l=sessions.loc[~(sessions.impressions.isnull()),'impressions'].tolist()\n",
    "\n",
    "#         imp_flat = [item for sublist in l for item in sublist]\n",
    "        \n",
    "#         return list(set(refs+imp_flat))\n",
    "    \n",
    "#     classes = get_classes(sessions)\n",
    "\n",
    "        \n",
    "    \n",
    "    print('Train encoders and save')\n",
    "    encoders = {}\n",
    "    for col in columns_to_encode:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        encoders[col]=le.fit(sessions[col])\n",
    "        print(encoders[col].classes_)\n",
    "    #     val_wide[col]=encoders[col].transform(val_wide[col])\n",
    "\n",
    "    with open(config['le_pickle_path'], 'wb') as handle:\n",
    "        pickle.dump(encoders, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # with open(config['le_pickle_path'], 'rb') as handle:\n",
    "    #     b = pickle.load(handle)\n",
    "    \n",
    "    print('Get Splits')\n",
    "\n",
    "    train = sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False)] \\\n",
    "                        .drop(['impressions','prices','is_train','is_validation','target'],axis=1) \\\n",
    "                        .reset_index(drop=True)\n",
    "\n",
    "    test = sessions.loc[sessions.is_train==False] \\\n",
    "                       .drop(['is_train','is_validation'],axis=1) \\\n",
    "                       .reset_index(drop=True)\n",
    "\n",
    "    val = sessions.loc[(sessions.is_train==True) & (sessions.is_validation==True)] \\\n",
    "                       .drop(['is_train','is_validation'],axis=1) \\\n",
    "                       .reset_index(drop=True)\n",
    "\n",
    "    print('train',train.shape)\n",
    "    print('test',test.shape)\n",
    "    print('val',val.shape)\n",
    "\n",
    "    print('train',train.dtypes)\n",
    "    print('test',test.dtypes)\n",
    "    print('val',val.dtypes)\n",
    "\n",
    "\n",
    "    print('Save either test or val')\n",
    "    if config['use_validation']:\n",
    "        val.to_pickle(config['val_pickle_path'])\n",
    "    else:\n",
    "        test.to_pickle(config['test_pickle_path'])\n",
    "\n",
    "\n",
    "    print('delete session, test and val')\n",
    "    try:\n",
    "        del sessions, test, val\n",
    "    except NameError:\n",
    "        pass\n",
    "    else:\n",
    "        gc.collect()\n",
    "     \n",
    "    \n",
    "    print('Calc. train_wide')\n",
    "    train_wide = process_train_naives_bayes(data=train, metadata=meta, encoders=encoders, config=config)\n",
    "    \n",
    "    print('delete train')\n",
    "    try:\n",
    "        del train\n",
    "    except NameError:\n",
    "        pass\n",
    "    else:\n",
    "        gc.collect()\n",
    "        \n",
    "    if config['drop_no_references']:\n",
    "        train_wide_not_allnull=train_wide.loc[train_wide.iloc[:,:config['session_length']].sum(axis=1)>0].copy()\n",
    "    else:\n",
    "        train_wide_not_allnull=train_wide.copy()\n",
    "\n",
    "    try:\n",
    "        del train_wide\n",
    "    except NameError:\n",
    "        pass\n",
    "    else:\n",
    "        gc.collect()\n",
    "    \n",
    "    print('save train_wide and classes')\n",
    "    train_wide_not_allnull.to_pickle(config['train_wide_pickle_path'])\n",
    "    \n",
    "    with open(config['classes_pickle_path'], 'wb') as fp:\n",
    "        pickle.dump(classes, fp)\n",
    "\n",
    "else:\n",
    "    print('loading train_wide, classes and encoders...')\n",
    "    \n",
    "    train_wide_not_allnull=pd.read_pickle(config['train_wide_pickle_path'])\n",
    "    \n",
    "    with open (config['classes_pickle_path'], 'rb') as fp:\n",
    "        classes = pickle.load(fp)\n",
    "    \n",
    "    with open(config['le_pickle_path'], 'rb') as handle:\n",
    "        encoders = pickle.load(handle)\n",
    "\n",
    "    print('done')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41384, 160)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_wide_not_allnull.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config['drop_no_references']:\n",
    "#     train_wide_not_allnull=train_wide[(train_wide.iloc[:,0:((2*config['session_length'])-1)].T != 0).any()].copy()\n",
    "# else:\n",
    "#     train_wide_not_allnull=train_wide.copy()\n",
    "\n",
    "# try:\n",
    "#     del train_wide\n",
    "# except NameError:\n",
    "#     pass\n",
    "# else:\n",
    "#     gc.collect()\n",
    "\n",
    "# train_wide_not_allnull.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_type|1</th>\n",
       "      <th>reference|1</th>\n",
       "      <th>y</th>\n",
       "      <th>1 Star</th>\n",
       "      <th>2 Star</th>\n",
       "      <th>3 Star</th>\n",
       "      <th>4 Star</th>\n",
       "      <th>5 Star</th>\n",
       "      <th>Accessible Hotel</th>\n",
       "      <th>Accessible Parking</th>\n",
       "      <th>...</th>\n",
       "      <th>Terrace (Hotel)</th>\n",
       "      <th>Theme Hotel</th>\n",
       "      <th>Towels</th>\n",
       "      <th>Very Good Rating</th>\n",
       "      <th>Volleyball</th>\n",
       "      <th>Washing Machine</th>\n",
       "      <th>Water Slide</th>\n",
       "      <th>Wheelchair Accessible</th>\n",
       "      <th>WiFi (Public Areas)</th>\n",
       "      <th>WiFi (Rooms)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0017FIR55K7R_dbd605dbee1e5_3</th>\n",
       "      <td>1</td>\n",
       "      <td>2745276</td>\n",
       "      <td>2745276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0017FIR55K7R_dbd605dbee1e5_4</th>\n",
       "      <td>1</td>\n",
       "      <td>2745276</td>\n",
       "      <td>2745276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0017FIR55K7R_dbd605dbee1e5_5</th>\n",
       "      <td>1</td>\n",
       "      <td>2745276</td>\n",
       "      <td>2745276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0025B8BU0NYP_37bfe437f8b89_43</th>\n",
       "      <td>6</td>\n",
       "      <td>4773134</td>\n",
       "      <td>2829394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002BISXP1U1Q_8cd721ffb8e03_2</th>\n",
       "      <td>3</td>\n",
       "      <td>503071</td>\n",
       "      <td>503071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               action_type|1 reference|1        y  1 Star  \\\n",
       "key                                                                         \n",
       "0017FIR55K7R_dbd605dbee1e5_3               1     2745276  2745276     0.0   \n",
       "0017FIR55K7R_dbd605dbee1e5_4               1     2745276  2745276     0.0   \n",
       "0017FIR55K7R_dbd605dbee1e5_5               1     2745276  2745276     0.0   \n",
       "0025B8BU0NYP_37bfe437f8b89_43              6     4773134  2829394     0.0   \n",
       "002BISXP1U1Q_8cd721ffb8e03_2               3      503071   503071     0.0   \n",
       "\n",
       "                               2 Star  3 Star  4 Star  5 Star  \\\n",
       "key                                                             \n",
       "0017FIR55K7R_dbd605dbee1e5_3      0.0     0.0     0.0     0.0   \n",
       "0017FIR55K7R_dbd605dbee1e5_4      0.0     0.0     0.0     0.0   \n",
       "0017FIR55K7R_dbd605dbee1e5_5      0.0     0.0     0.0     0.0   \n",
       "0025B8BU0NYP_37bfe437f8b89_43     0.0     0.0     0.0     1.0   \n",
       "002BISXP1U1Q_8cd721ffb8e03_2      0.0     1.0     0.0     0.0   \n",
       "\n",
       "                               Accessible Hotel  Accessible Parking  ...  \\\n",
       "key                                                                  ...   \n",
       "0017FIR55K7R_dbd605dbee1e5_3                0.0                 0.0  ...   \n",
       "0017FIR55K7R_dbd605dbee1e5_4                0.0                 0.0  ...   \n",
       "0017FIR55K7R_dbd605dbee1e5_5                0.0                 0.0  ...   \n",
       "0025B8BU0NYP_37bfe437f8b89_43               1.0                 1.0  ...   \n",
       "002BISXP1U1Q_8cd721ffb8e03_2                1.0                 1.0  ...   \n",
       "\n",
       "                               Terrace (Hotel)  Theme Hotel  Towels  \\\n",
       "key                                                                   \n",
       "0017FIR55K7R_dbd605dbee1e5_3               1.0          0.0     0.0   \n",
       "0017FIR55K7R_dbd605dbee1e5_4               1.0          0.0     0.0   \n",
       "0017FIR55K7R_dbd605dbee1e5_5               1.0          0.0     0.0   \n",
       "0025B8BU0NYP_37bfe437f8b89_43              1.0          0.0     1.0   \n",
       "002BISXP1U1Q_8cd721ffb8e03_2               1.0          0.0     0.0   \n",
       "\n",
       "                               Very Good Rating  Volleyball  Washing Machine  \\\n",
       "key                                                                            \n",
       "0017FIR55K7R_dbd605dbee1e5_3                0.0         0.0              1.0   \n",
       "0017FIR55K7R_dbd605dbee1e5_4                0.0         0.0              1.0   \n",
       "0017FIR55K7R_dbd605dbee1e5_5                0.0         0.0              1.0   \n",
       "0025B8BU0NYP_37bfe437f8b89_43               1.0         1.0              0.0   \n",
       "002BISXP1U1Q_8cd721ffb8e03_2                0.0         0.0              0.0   \n",
       "\n",
       "                               Water Slide  Wheelchair Accessible  \\\n",
       "key                                                                 \n",
       "0017FIR55K7R_dbd605dbee1e5_3           0.0                    0.0   \n",
       "0017FIR55K7R_dbd605dbee1e5_4           0.0                    0.0   \n",
       "0017FIR55K7R_dbd605dbee1e5_5           0.0                    0.0   \n",
       "0025B8BU0NYP_37bfe437f8b89_43          0.0                    1.0   \n",
       "002BISXP1U1Q_8cd721ffb8e03_2           0.0                    1.0   \n",
       "\n",
       "                               WiFi (Public Areas)  WiFi (Rooms)  \n",
       "key                                                               \n",
       "0017FIR55K7R_dbd605dbee1e5_3                   1.0           0.0  \n",
       "0017FIR55K7R_dbd605dbee1e5_4                   1.0           0.0  \n",
       "0017FIR55K7R_dbd605dbee1e5_5                   1.0           0.0  \n",
       "0025B8BU0NYP_37bfe437f8b89_43                  1.0           1.0  \n",
       "002BISXP1U1Q_8cd721ffb8e03_2                   1.0           1.0  \n",
       "\n",
       "[5 rows x 160 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_wide_not_allnull.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference|1\n",
      "action_type|1\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop=([])\n",
    "for i in range(config['session_length']):\n",
    "    print('reference|' + str(i+1))\n",
    "    cols_to_drop=np.append(cols_to_drop,'reference|' + str(i+1))\n",
    "    if config['drop_action_type']:\n",
    "        print('action_type|' + str(i+1))\n",
    "        cols_to_drop=np.append(cols_to_drop,'action_type|' + str(i+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 4\n",
      "[0, 9999]\n",
      "0017FIR55K7R_dbd605dbee1e5_3\n",
      "7CCX84QYA7RZ_7d1088fa7e2c3_38\n",
      "1 of 4\n",
      "[10000, 19999]\n",
      "7CE0S4E64ZMZ_56e01e69d5018_3\n",
      "FPYETWGI3KT3_3749da5becc70_65\n",
      "2 of 4\n",
      "[20000, 29999]\n",
      "FPYETWGI3KT3_3749da5becc70_85\n",
      "P3JCVZFA5Q38_74cc0ced1d42b_2\n",
      "3 of 4\n",
      "[30000, 39999]\n",
      "P3OV66293CG3_ff507ffe93f73_35\n",
      "YMW2YM5X5O6E_739781d4f77f4_7\n",
      "YMW2YM5X5O6E_739781d4f77f4_9\n",
      "Seconds elapsed  : 73.2729127407074\n"
     ]
    }
   ],
   "source": [
    "if not config['load_fitted_model']:\n",
    "    start = time.time()\n",
    "    \n",
    "    clf=BernoulliNB()\n",
    "    \n",
    "    \n",
    "    for i in range(train_wide_not_allnull.shape[0]//config['train_session_chunksize']):\n",
    "        print('%s of %s' % (i,train_wide_not_allnull.shape[0]//config['train_session_chunksize']))\n",
    "        idxi = i*config['train_session_chunksize']\n",
    "        idxf = ((i+1)*config['train_session_chunksize'])-1\n",
    "        print([idxi,idxf])\n",
    "        \n",
    "        if i==0:\n",
    "            print(train_wide_not_allnull.iloc[idxi:idxf,:].index[0])\n",
    "            print(train_wide_not_allnull.iloc[idxi:idxf,:].index[-1])\n",
    "            \n",
    "            clf.partial_fit(train_wide_not_allnull.iloc[idxi:idxf,:] \\\n",
    "                        .drop(np.append(cols_to_drop,'y'),axis=1), \\\n",
    "                               np.ravel(train_wide_not_allnull.iloc[idxi:idxf,:].y), \\\n",
    "                               classes = classes)\n",
    "            \n",
    "        else:\n",
    "            print(train_wide_not_allnull.iloc[idxi:idxf,:].index[0])\n",
    "            print(train_wide_not_allnull.iloc[idxi:idxf,:].index[-1])\n",
    "            \n",
    "            clf.partial_fit(train_wide_not_allnull.iloc[idxi:idxf,:] \\\n",
    "                        .drop(np.append(cols_to_drop,'y'),axis=1), \\\n",
    "                               np.ravel(train_wide_not_allnull.iloc[idxi:idxf,:].y))\n",
    "            \n",
    "    \n",
    "    if train_wide_not_allnull.shape[0]%config['train_session_chunksize']!=0:\n",
    "        if train_wide_not_allnull.shape[0]//config['train_session_chunksize']==0:\n",
    "            clf.partial_fit(train_wide_not_allnull \\\n",
    "                        .drop(np.append(cols_to_drop,'y'),axis=1), \\\n",
    "                               np.ravel(train_wide_not_allnull.y), \\\n",
    "                               classes = classes)\n",
    "            \n",
    "        else:\n",
    "            idx = train_wide_not_allnull.shape[0]%config['train_session_chunksize']\n",
    "\n",
    "            print(train_wide_not_allnull.iloc[-idx:,:].index[0])\n",
    "\n",
    "            clf.partial_fit(train_wide_not_allnull.iloc[-idx:,:] \\\n",
    "                                .drop(np.append(cols_to_drop,'y'),axis=1), \\\n",
    "                                       np.ravel(train_wide_not_allnull.iloc[-idx:,:].y))\n",
    "\n",
    "    stop = time.time()\n",
    "    print('Seconds elapsed  :', stop-start) \n",
    "    \n",
    "    # Output a pickle file for the model\n",
    "    joblib.dump(clf, config['model_pickle_path'])\n",
    "    \n",
    "    del train_wide_not_allnull\n",
    "    \n",
    "else:\n",
    "    clf=joblib.load(config['model_pickle_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on test data clickouts that have a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds elapsed  : 12.552316665649414\n"
     ]
    }
   ],
   "source": [
    "if config['train_model_on_test_data']:\n",
    "    \n",
    "    if config['use_validation']:\n",
    "        data=pd.read_pickle(config['val_pickle_path'])\n",
    "    else:\n",
    "        data=pd.read_pickle(config['test_pickle_path'])\n",
    "    \n",
    "    # remove empty reference rows\n",
    "    data=data.loc[~(data.reference.isnull())]\n",
    "    train_wide=process_train_naives_bayes(data=data, metadata=meta, encoders=encoders, config=config)\n",
    "    \n",
    "    try:\n",
    "        del data\n",
    "    except NameError:\n",
    "        pass\n",
    "    else:\n",
    "        gc.collect()\n",
    "    \n",
    "    if config['drop_no_references']:\n",
    "        train_wide_not_allnull=train_wide.loc[train_wide.iloc[:,:config['session_length']].sum(axis=1)>0].copy()\n",
    "    else:\n",
    "        train_wide_not_allnull=train_wide.copy()\n",
    "        \n",
    "    try:\n",
    "        del train_wide\n",
    "    except NameError:\n",
    "        pass\n",
    "    else:\n",
    "        gc.collect()\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for i in range(train_wide_not_allnull.shape[0]//config['train_session_chunksize']):\n",
    "        print('%s of %s' % (i,train_wide_not_allnull.shape[0]//config['train_session_chunksize']))\n",
    "        idxi = i*config['train_session_chunksize']\n",
    "        idxf = ((i+1)*config['train_session_chunksize'])-1\n",
    "        print([idxi,idxf])\n",
    "        \n",
    "        if i==0:\n",
    "            print(train_wide_not_allnull.iloc[idxi:idxf,:].index[0])\n",
    "            print(train_wide_not_allnull.iloc[idxi:idxf,:].index[-1])\n",
    "            \n",
    "            clf.partial_fit(train_wide_not_allnull.iloc[idxi:idxf,:] \\\n",
    "                        .drop(np.append(cols_to_drop,'y'),axis=1), \\\n",
    "                               np.ravel(train_wide_not_allnull.iloc[idxi:idxf,:].y), \\\n",
    "                               classes = classes)\n",
    "            \n",
    "        else:\n",
    "            print(train_wide_not_allnull.iloc[idxi:idxf,:].index[0])\n",
    "            print(train_wide_not_allnull.iloc[idxi:idxf,:].index[-1])\n",
    "            \n",
    "            clf.partial_fit(train_wide_not_allnull.iloc[idxi:idxf,:] \\\n",
    "                        .drop(np.append(cols_to_drop,'y'),axis=1), \\\n",
    "                               np.ravel(train_wide_not_allnull.iloc[idxi:idxf,:].y))\n",
    "            \n",
    "    \n",
    "    if train_wide_not_allnull.shape[0]%config['train_session_chunksize']!=0:\n",
    "        if train_wide_not_allnull.shape[0]//config['train_session_chunksize']==0:\n",
    "            clf.partial_fit(train_wide_not_allnull \\\n",
    "                        .drop(np.append(cols_to_drop,'y'),axis=1), \\\n",
    "                               np.ravel(train_wide_not_allnull.y), \\\n",
    "                               classes = classes)\n",
    "            \n",
    "        else:\n",
    "            idx = train_wide_not_allnull.shape[0]%config['train_session_chunksize']\n",
    "\n",
    "            print(train_wide_not_allnull.iloc[-idx:,:].index[0])\n",
    "\n",
    "            clf.partial_fit(train_wide_not_allnull.iloc[-idx:,:] \\\n",
    "                                .drop(np.append(cols_to_drop,'y'),axis=1), \\\n",
    "                                       np.ravel(train_wide_not_allnull.iloc[-idx:,:].y))\n",
    "\n",
    "    stop = time.time()\n",
    "    print('Seconds elapsed  :', stop-start) \n",
    "    \n",
    "#     # Output a pickle file for the model\n",
    "#     joblib.dump(clf, config['model_pickle_path'])\n",
    "    \n",
    "    del train_wide_not_allnull\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get test data in wide format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split for partial fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['use_validation']:\n",
    "    data=pd.read_pickle(config['val_pickle_path'])\n",
    "else:\n",
    "    data=pd.read_pickle(config['test_pickle_path'])\n",
    "\n",
    "n_clickouts_test = data.loc[(data.action_type=='clickout item') & (data.reference.isnull()),'step'].count()\n",
    "\n",
    "val_wide = process_test_naives_bayes(data=data, metadata=meta, encoders=encoders, config=config)\n",
    "val_wide=val_wide.loc[val_wide.impressions!=0]\n",
    "\n",
    "\n",
    "try:\n",
    "    del data\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    gc.collect()\n",
    "\n",
    "if config['drop_no_references']:#\n",
    "    val_wide_not_allnull=val_wide.loc[val_wide.iloc[:,:config['session_length']].sum(axis=1)>0].copy()\n",
    "else:\n",
    "    val_wide_not_allnull=val_wide.copy()\n",
    "\n",
    "val_wide_not_allnull['impressions']=val_wide_not_allnull['impressions'].str.split('\\\\|')\n",
    "\n",
    "try:\n",
    "    del val_wide\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create long table for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imp=val_wide_not_allnull[['impressions','target']].reset_index().copy()\n",
    "\n",
    "val_imp_long=val_imp.impressions.apply(pd.Series) \\\n",
    "    .merge(val_imp, right_index = True, left_index = True) \\\n",
    "    .drop([\"impressions\"], axis = 1)  \\\n",
    "    .melt(id_vars = ['key','target'], value_name = \"impressions\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna() \\\n",
    "    .sort_values('key') \\\n",
    "    .reset_index(drop=True) \\\n",
    "    .copy()\n",
    "\n",
    "try:\n",
    "    del val_imp\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get row, col indexes for extracting probabilities from pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index=val_imp_long['key'].copy()\n",
    "col_index=val_imp_long['impressions'].copy()\n",
    "\n",
    "mask_in_class = col_index.isin(clf.classes_) #True is in class\n",
    "col_index.loc[~mask_in_class]='not_in_class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do predict and munge output on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6549, 162)\n",
      "6549\n",
      "0 of 1\n",
      "Seconds elapsed  : 17.109497547149658\n"
     ]
    }
   ],
   "source": [
    "print(val_wide_not_allnull.shape)\n",
    "\n",
    "n_splits = round(val_wide_not_allnull.shape[0]/config['parts_nrows_test'])\n",
    "if n_splits==0:\n",
    "    n_splits=1\n",
    "    index_split = val_wide_not_allnull.shape[0]\n",
    "else:\n",
    "    index_split = round(val_wide_not_allnull.shape[0]/n_splits)\n",
    "    \n",
    "print(index_split)\n",
    "\n",
    "start = time.time()\n",
    "impression_probs=([])\n",
    "for i in range(n_splits): # n_splits\n",
    "    print('%s of %s' % (i,n_splits))\n",
    "    if (i+1)==n_splits:\n",
    "        pred=clf.predict_proba(val_wide_not_allnull.iloc[(index_split*(i)):,:] \\\n",
    "                                         .drop(np.append(cols_to_drop,['timestamp', 'impressions','target']),axis=1))\n",
    "        \n",
    "        preddf=pd.DataFrame(data=pred,columns=clf.classes_,index=val_wide_not_allnull.iloc[(index_split*(i)):,:].index)\n",
    "        preddf['not_in_class']=0 # need to return 0 in case impression not in clf.classes_\n",
    "        \n",
    "        first_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):,:].head(1).index\n",
    "        first_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(first_key_last_pred)].index.min()\n",
    "\n",
    "        last_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):,:].tail(1).index\n",
    "        last_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(last_key_last_pred)].index.max()\n",
    "\n",
    "        row_index[first_index_for_row_col:(last_index_for_row_col+1)]\n",
    "        \n",
    "        impression_probs=np.append(impression_probs,preddf.lookup(row_index[first_index_for_row_col:(last_index_for_row_col+1)], \\\n",
    "                                              col_index[first_index_for_row_col:(last_index_for_row_col+1)]),axis=0)\n",
    "        \n",
    "    else:\n",
    "        pred=clf.predict_proba(val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:] \\\n",
    "                                         .drop(np.append(cols_to_drop,['timestamp', 'impressions','target']),axis=1))\n",
    "        \n",
    "        \n",
    "        preddf=pd.DataFrame(data=pred,columns=clf.classes_,index=val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:].index)\n",
    "        preddf['not_in_class']=0 # need to return 0 in case impression not in clf.classes_\n",
    "        \n",
    "        first_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:].head(1).index\n",
    "        first_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(first_key_last_pred)].index.min()\n",
    "\n",
    "        last_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:].tail(1).index\n",
    "        last_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(last_key_last_pred)].index.max()\n",
    "\n",
    "        row_index[first_index_for_row_col:(last_index_for_row_col+1)]\n",
    "        \n",
    "        impression_probs=np.append(impression_probs,preddf.lookup(row_index[first_index_for_row_col:(last_index_for_row_col+1)], \\\n",
    "                                              col_index[first_index_for_row_col:(last_index_for_row_col+1)]),axis=0)\n",
    "        \n",
    "stop = time.time()\n",
    "print('Seconds elapsed  :', stop-start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Munge to get rank and RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imp_long['probs'] = impression_probs\n",
    "\n",
    "val_imp_long.sort_values(['key','probs'],ascending=[True,False],inplace=True)\n",
    "\n",
    "val_imp_long['rank'] = 1\n",
    "val_imp_long['rank'] = val_imp_long.groupby('key')['rank'].cumsum()\n",
    "\n",
    "val_imp_long['RR']=1/val_imp_long['rank']\n",
    "\n",
    "val_imp_long.to_csv(config['val_long_csv_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_imp_wide=val_imp_long.pivot(index='key',columns='rank',values=['impressions','probs']).copy()\n",
    "\n",
    "# collapse column multi index for ease of indexing\n",
    "rank_imp_wide.columns=rank_imp_wide.columns.map(lambda x: '|'.join([str(i) for i in x]))\n",
    "    \n",
    "rank_imp_wide=rank_imp_wide.join(val_wide_not_allnull['timestamp'], on='key')\n",
    "\n",
    "rank_imp_wide['timestamp']=rank_imp_wide['timestamp'].astype(np.int64)//10**9\n",
    "\n",
    "rank_imp_wide.reset_index(inplace=True)\n",
    "\n",
    "rank_imp_wide[['user_id','session_id','step']]=rank_imp_wide['key'].str.split('_',expand=True)\n",
    "\n",
    "rank_imp_wide.drop(['key'],axis=1,inplace=True)\n",
    "\n",
    "rank_imp_wide['item_recommendations'] = rank_imp_wide.iloc[:,:25].apply(\n",
    "    lambda x: ' '.join(x.dropna()),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "rank_imp_wide.drop(rank_imp_wide.columns[:25],axis=1,inplace=True)\n",
    "\n",
    "rank_imp_wide['item_probs'] = rank_imp_wide.iloc[:,:25].apply(\n",
    "    lambda x: ' '.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "rank_imp_wide.drop(rank_imp_wide.columns[:25],axis=1,inplace=True)\n",
    "\n",
    "cols=rank_imp_wide.columns.tolist()\n",
    "cols=cols[1:3]+cols[:1]+cols[3:4]+cols[-2:]\n",
    "rank_imp_wide=rank_imp_wide[cols]\n",
    "\n",
    "rank_imp_wide.head()\n",
    "\n",
    "rank_imp_wide.to_csv(config['output_meta_only_fiited_csv_path'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6255510502812318\n",
      "0.3352119653916944\n",
      "support MRR_NB:  2793\n",
      "support MRR_NB:  6544\n",
      "10336\n"
     ]
    }
   ],
   "source": [
    "if config['use_validation']:\n",
    "    MRR_NB=val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)) \\\n",
    "                           & (val_imp_long.probs!=0.0),'RR'].mean()\n",
    "    MRR_parse_imp=val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)),'RR'].mean()\n",
    "    \n",
    "    print(MRR_NB)\n",
    "    print(MRR_parse_imp)\n",
    "    print('support MRR_NB: ', str(val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)) & (val_imp_long.probs!=0.0),'RR'].count()))\n",
    "    print('support MRR_NB: ', str(val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)),'RR'].count()))\n",
    "    print(n_clickouts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6155831391969003\n",
      "1261\n"
     ]
    }
   ],
   "source": [
    "threshold=0.1\n",
    "\n",
    "sum_probs=val_imp_long.groupby('key').probs.sum()\n",
    "val_imp_sum=val_imp_long.join(sum_probs,on='key',rsuffix='_sum')\n",
    "print(val_imp_sum.loc[(val_imp_sum.probs_sum>threshold) & (val_imp_sum.target.astype(int)==val_imp_sum.impressions.astype(int))].RR.mean())\n",
    "print(val_imp_sum.loc[(val_imp_sum.probs_sum>threshold) & (val_imp_sum.target.astype(int)==val_imp_sum.impressions.astype(int))].RR.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add lost sessions back\n",
    "Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['drop_no_references']:\n",
    "    if config['use_validation']:\n",
    "        data=pd.read_pickle(config['val_pickle_path'])\n",
    "    else:\n",
    "        data=pd.read_pickle(config['test_pickle_path'])\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    val_wide = process_test_naives_bayes(data=data, metadata=meta, encoders=encoders, config=config)\n",
    "    val_wide=val_wide.loc[val_wide.impressions!=0]\n",
    "\n",
    "    try:\n",
    "        del test\n",
    "    except NameError:\n",
    "        pass\n",
    "    else:\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    val_wide_allnull=val_wide.loc[val_wide.iloc[:,:config['session_length']].sum(axis=1)==0].copy()\n",
    "    \n",
    "    try:\n",
    "        del val_wide\n",
    "    except NameError:\n",
    "        pass\n",
    "    else:\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Munge into same format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>step</th>\n",
       "      <th>item_recommendations</th>\n",
       "      <th>item_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00M5AMMLYQG5</td>\n",
       "      <td>8fd417ebd2d5b</td>\n",
       "      <td>1541062793</td>\n",
       "      <td>1</td>\n",
       "      <td>9882016 106769 48219 147360 7333050 3533848 75...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00WWQYCIDPUF</td>\n",
       "      <td>a8962895bad41</td>\n",
       "      <td>1541064252</td>\n",
       "      <td>6</td>\n",
       "      <td>7929996 8767518 5742058 7195114 7325018 196079...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0168S4C5E60K</td>\n",
       "      <td>016150a298e6e</td>\n",
       "      <td>1541063179</td>\n",
       "      <td>9</td>\n",
       "      <td>42109 42127 42270 137704 1577271 1750769 42146...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01BS6LPQ5JTX</td>\n",
       "      <td>2a35cec32ede2</td>\n",
       "      <td>1541063117</td>\n",
       "      <td>1</td>\n",
       "      <td>127745 43084 1257131 1797067 1848773 5452918 4...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01MTZ6S2OHUA</td>\n",
       "      <td>2d123642df132</td>\n",
       "      <td>1541059612</td>\n",
       "      <td>10</td>\n",
       "      <td>1277246 1341812 1550767 1388724 2187086 124244...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id     session_id   timestamp step  \\\n",
       "0  00M5AMMLYQG5  8fd417ebd2d5b  1541062793    1   \n",
       "1  00WWQYCIDPUF  a8962895bad41  1541064252    6   \n",
       "2  0168S4C5E60K  016150a298e6e  1541063179    9   \n",
       "3  01BS6LPQ5JTX  2a35cec32ede2  1541063117    1   \n",
       "4  01MTZ6S2OHUA  2d123642df132  1541059612   10   \n",
       "\n",
       "                                item_recommendations  \\\n",
       "0  9882016 106769 48219 147360 7333050 3533848 75...   \n",
       "1  7929996 8767518 5742058 7195114 7325018 196079...   \n",
       "2  42109 42127 42270 137704 1577271 1750769 42146...   \n",
       "3  127745 43084 1257131 1797067 1848773 5452918 4...   \n",
       "4  1277246 1341812 1550767 1388724 2187086 124244...   \n",
       "\n",
       "                                          item_probs  \n",
       "0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "1  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "2       0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0   \n",
       "3  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "4               0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0   "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_ordered = ['user_id', 'session_id', 'timestamp', 'step', 'item_recommendations',  'item_probs']\n",
    "\n",
    "val_wide_allnull=val_wide_allnull[['timestamp','impressions']]\n",
    "val_wide_allnull['timestamp']=val_wide_allnull['timestamp'].astype(np.int64)//10**9\n",
    "val_wide_allnull.reset_index(inplace=True)\n",
    "val_wide_allnull[['user_id','session_id','step']]=val_wide_allnull['key'].str.split('_',expand=True)\n",
    "val_wide_allnull.drop(['key'],axis=1,inplace=True)\n",
    "val_wide_allnull['item_probs']=val_wide_allnull['impressions'].str.replace('[0-9]+\\\\||[0-9]+','0 ')\n",
    "val_wide_allnull['impressions']=val_wide_allnull['impressions'].str.replace('\\\\|',' ')\n",
    "val_wide_allnull.rename(columns={'impressions':'item_recommendations'},inplace=True)\n",
    "\n",
    "\n",
    "columns_ordered = ['user_id', 'session_id', 'timestamp', 'step', 'item_recommendations',  'item_probs']\n",
    "val_wide_allnull=val_wide_allnull.reindex(columns=columns_ordered)\n",
    "\n",
    "val_wide_allnull.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "append with NB fitted test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=rank_imp_wide.append(val_wide_allnull,ignore_index=True,sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.drop('item_probs',axis=1).to_csv(config['output_recsys_csv_path'],index=False)\n",
    "output.to_csv(config['output_meta_csv_path'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10336, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3787, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_wide_allnull.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6549, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_imp_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_wide_allnull.shape[0]+rank_imp_wide.shape[0]-n_clickouts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clickouts_test-output.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
