{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import time\n",
    "\n",
    "# %% Imports\n",
    "from pathlib import Path\n",
    "import sys\n",
    "root_dir = Path().resolve()\n",
    "sys.path.append(str(root_dir / 'src'))\n",
    "\n",
    "from recsys_common import *\n",
    "from recsys_naive_bayes_processing import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config= {\n",
    "    'save_train_test_val': True,\n",
    "    'load_fitted_model': False,\n",
    "    \n",
    "    'use_subset': True,\n",
    "    'subset_frac': 0.05,\n",
    "    'use_validation': True,\n",
    "    'validation_frac': 0.25,\n",
    "    'reference_to_nan_frac': 1,\n",
    "    'reference_to_nan_seed': 1234,\n",
    "    \n",
    "    'session_length': 1,\n",
    "    'drop_no_references': True,\n",
    "    \n",
    "    'train_session_chunksize': 5000,\n",
    "    'parts_nrows_test': 5000,\n",
    "    'parts_path_to_data': root_dir / 'cache' / 'parts',\n",
    "    'data_path': root_dir / 'cache'\n",
    "    }\n",
    "\n",
    "\n",
    "if not config['use_subset']:\n",
    "    config['subset_frac']=1\n",
    "\n",
    "config['le_pickle_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) + '_le.pickle')\n",
    "config['train_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_train.csv')\n",
    "config['train_last_step_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_train_last_step.csv')\n",
    "config['test_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_test.csv')\n",
    "config['val_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_val.csv')\n",
    "config['model_pickle_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_model.pickle')\n",
    "config['val_long_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_val_long.csv')\n",
    "config['output_recsys_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_output_recsys.csv')\n",
    "config['output_meta_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_output_meta.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta=get_metadata()\n",
    "# meta.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['item_id']=meta['item_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sessions\n",
      "Filter session with no clickout\n",
      "user_id                         object\n",
      "session_id                      object\n",
      "timestamp          datetime64[ns, UTC]\n",
      "step                             int64\n",
      "action_type                     object\n",
      "reference                       object\n",
      "platform                        object\n",
      "city                            object\n",
      "device                          object\n",
      "current_filters                 object\n",
      "impressions                     object\n",
      "prices                          object\n",
      "is_validation                     bool\n",
      "is_train                          bool\n",
      "target                          object\n",
      "dtype: object\n",
      "Quick unit test\n",
      "893499\n",
      "893499\n",
      "10336\n",
      "0\n",
      "0\n",
      "19995\n",
      "10336\n",
      "59661\n",
      "0\n",
      "26345\n",
      "12856\n",
      "Train encoders and save\n",
      "['change of sort order' 'clickout item' 'filter selection'\n",
      " 'interaction item deals' 'interaction item image' 'interaction item info'\n",
      " 'interaction item rating' 'search for destination' 'search for item'\n",
      " 'search for poi']\n",
      "['AA' 'AE' 'AR' 'AT' 'AU' 'BE' 'BG' 'BR' 'CA' 'CH' 'CL' 'CN' 'CO' 'CZ'\n",
      " 'DE' 'DK' 'EC' 'ES' 'FI' 'FR' 'GR' 'HK' 'HR' 'HU' 'ID' 'IE' 'IL' 'IN'\n",
      " 'IT' 'JP' 'KR' 'MX' 'MY' 'NL' 'NO' 'NZ' 'PE' 'PH' 'PL' 'PT' 'RO' 'RS'\n",
      " 'RU' 'SE' 'SG' 'SI' 'SK' 'TH' 'TR' 'TW' 'UK' 'US' 'UY' 'VN' 'ZA']\n",
      "['A Teixeira, Spain' 'Aachen, Germany' 'Aadorf, Switzerland' ...\n",
      " 'Żarki, Poland' 'Žabljak, Montenegro' 'Žilina, Slovakia']\n",
      "['desktop' 'mobile' 'tablet']\n",
      "Get Splits\n",
      "train (537963, 10)\n",
      "test (200069, 13)\n",
      "val (155467, 13)\n",
      "Save either test or val\n",
      "delete session, test and val\n",
      "save train\n",
      "delete train\n"
     ]
    }
   ],
   "source": [
    "if config['save_train_test_val']:\n",
    "    print('Getting sessions')\n",
    "    sessions=get_sessions(config['use_subset'],\n",
    "                          config['subset_frac'],\n",
    "                          config['use_validation'],\n",
    "                          config['validation_frac'],\n",
    "                          config['reference_to_nan_frac'],\n",
    "                          config['reference_to_nan_seed'])\n",
    "\n",
    "    print('Filter session with no clickout')\n",
    "    if (not config['use_validation']) & (not config['use_subset']):\n",
    "        print('filtering sessions with clickout')\n",
    "        sessions=filter_sessions_with_no_clicks(sessions)\n",
    "\n",
    "#     print('Split impressions and prices')\n",
    "#     sessions['impressions']=sessions['impressions'].str.split('\\\\|')\n",
    "\n",
    "#     sessions['prices']=sessions['prices'].str.split('\\\\|')\n",
    "\n",
    "    print(sessions.dtypes)\n",
    "    # sessions.head()\n",
    "\n",
    "    print('Quick unit test')\n",
    "\n",
    "    print(len(sessions.index))\n",
    "    print(len(sessions.index.unique()))\n",
    "\n",
    "    if not config['use_validation']:\n",
    "        sessions['is_validation']=False\n",
    "        sessions['target']=np.NaN\n",
    "\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==True),'target'].count())\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False),'target'].count())\n",
    "    print(sessions.loc[(sessions.is_train==False) & (sessions.is_validation==False),'target'].count())\n",
    "\n",
    "    print(sessions.loc[(sessions.is_validation==True) & (sessions.action_type=='clickout item'),'step'].count())\n",
    "    print(sessions.loc[(sessions.is_validation==True) & (sessions.action_type=='clickout item') & (sessions.reference.isnull()),'step'].count())\n",
    "\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False) & (sessions.action_type=='clickout item') ,'step'].count())\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False) & (sessions.action_type=='clickout item') & (sessions.reference.isnull()),'step'].count())\n",
    "\n",
    "    print(sessions.loc[(sessions.is_train==False) & (sessions.is_validation==False) & (sessions.action_type=='clickout item'),'step'].count())\n",
    "    print(sessions.loc[(sessions.is_train==False) & (sessions.is_validation==False) & (sessions.action_type=='clickout item') & (sessions.reference.isnull()),'step'].count())\n",
    "\n",
    "    print('Train encoders and save')\n",
    "\n",
    "    columns_to_encode = ['action_type','platform','city','device']\n",
    "\n",
    "    encoders = {}\n",
    "    for col in columns_to_encode:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        encoders[col]=le.fit(sessions[col])\n",
    "        print(encoders[col].classes_)\n",
    "    #     val_wide[col]=encoders[col].transform(val_wide[col])\n",
    "\n",
    "    with open(config['le_pickle_path'], 'wb') as handle:\n",
    "        pickle.dump(encoders, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # with open(config['le_pickle_path'], 'rb') as handle:\n",
    "    #     b = pickle.load(handle)\n",
    "\n",
    "    print('Get Splits')\n",
    "\n",
    "    train = sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False)] \\\n",
    "                        .drop(['impressions','prices','is_train','is_validation','target'],axis=1) \\\n",
    "                        .reset_index(drop=True)\n",
    "\n",
    "    test = sessions.loc[sessions.is_train==False] \\\n",
    "                       .drop(['is_train','is_validation'],axis=1) \\\n",
    "                       .reset_index(drop=True)\n",
    "\n",
    "    val = sessions.loc[(sessions.is_train==True) & (sessions.is_validation==True)] \\\n",
    "                       .drop(['is_train','is_validation'],axis=1) \\\n",
    "                       .reset_index(drop=True)\n",
    "\n",
    "    print('train',train.shape)\n",
    "    print('test',test.shape)\n",
    "    print('val',val.shape)\n",
    "\n",
    "\n",
    "    print('Save either test or val')\n",
    "    if config['use_validation']:\n",
    "        val.to_csv(config['val_csv_path'])\n",
    "    else:\n",
    "        test.to_csv(config['test_csv_path'])\n",
    "\n",
    "\n",
    "    print('delete session, test and val')\n",
    "    try:\n",
    "        del sessions, test, val\n",
    "    except NameError:\n",
    "        pass\n",
    "    else:\n",
    "        gc.collect()\n",
    "     \n",
    "    \n",
    "\n",
    "\n",
    "    print('save train')\n",
    "    train.to_csv(config['train_csv_path'])\n",
    "\n",
    "else:\n",
    "    print('loading train and encoders...')\n",
    "    \n",
    "    train=pd.read_csv(config['train_csv_path'])\n",
    "    train.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "    \n",
    "    with open(config['le_pickle_path'], 'rb') as handle:\n",
    "        encoders = pickle.load(handle)\n",
    "\n",
    "    print('done')\n",
    "    \n",
    "\n",
    "last_step_per_session=train.groupby('session_id',sort=False)['step'].max().reset_index().to_csv(config['train_last_step_csv_path'])\n",
    "\n",
    "names=train.columns\n",
    "\n",
    "# classes=list(set(train.loc[(train.action_type=='clickout item') & (train.step>1),'reference']))\n",
    "classes=list(set(train.loc[(train.action_type=='clickout item'),'reference']))\n",
    "\n",
    "print('delete train')\n",
    "try:\n",
    "    del train\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference|1\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop=([])\n",
    "for i in range(config['session_length']):\n",
    "    print('reference|' + str(i+1))\n",
    "    cols_to_drop=np.append(cols_to_drop,'reference|' + str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6386, 163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NewOffice3\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6598, 163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NewOffice3\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6760, 163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NewOffice3\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6956, 163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NewOffice3\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6737, 163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NewOffice3\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6541, 163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NewOffice3\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1396, 163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NewOffice3\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds elapsed  : 81.51977372169495\n"
     ]
    }
   ],
   "source": [
    "if not config['load_fitted_model']:\n",
    "    start = time.time()\n",
    "    reader=pd.read_csv(config['train_last_step_csv_path'], chunksize=config['train_session_chunksize'])\n",
    "\n",
    "    skiprows=0\n",
    "    clf=BernoulliNB()\n",
    "    for i,chunk in enumerate(reader):\n",
    "        nrows=chunk.step.sum()\n",
    "        train_part=pd.read_csv(config['train_csv_path'],header=0,skiprows=skiprows,nrows=nrows,names=names)\n",
    "\n",
    "        train_wide = process_train_naives_bayes(data=train_part, metadata=meta, session_length=config['session_length'], encode = True,encoders=encoders,cols_to_encode=list(encoders.keys()))\n",
    "\n",
    "        del train_part\n",
    "\n",
    "        if config['drop_no_references']:\n",
    "            train_wide_not_allnull=train_wide[(train_wide.iloc[:,0:(2*config['session_length']-1)].T != 0).all()].copy()\n",
    "        else:\n",
    "            train_wide_not_allnull=train_wide.copy()\n",
    "\n",
    "\n",
    "        del train_wide\n",
    "\n",
    "        print(train_wide_not_allnull.shape)\n",
    "        \n",
    "        if not train_wide_not_allnull.empty:\n",
    "            if i==0:\n",
    "                clf.partial_fit(train_wide_not_allnull \\\n",
    "                            .drop(np.append(cols_to_drop,'y'),axis=1), \\\n",
    "                                   np.ravel(train_wide_not_allnull.y), \\\n",
    "                                   classes = classes)\n",
    "            else:\n",
    "                clf.partial_fit(train_wide_not_allnull \\\n",
    "                            .drop(np.append(cols_to_drop,'y'),axis=1), \\\n",
    "                                   np.ravel(train_wide_not_allnull.y))\n",
    "\n",
    "        skiprows += nrows\n",
    "\n",
    "    stop = time.time()\n",
    "    print('Seconds elapsed  :', stop-start) \n",
    "    \n",
    "    # Output a pickle file for the model\n",
    "    joblib.dump(clf, config['model_pickle_path'])\n",
    "    \n",
    "    del train_wide_not_allnull\n",
    "    \n",
    "else:\n",
    "    clf=joblib.load(config['model_pickle_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get test data in wide format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split for partial fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['use_validation']:\n",
    "    data=pd.read_csv(config['val_csv_path'],parse_dates=['timestamp'])\n",
    "else:\n",
    "    data=pd.read_csv(config['test_csv_path'],parse_dates=['timestamp'])\n",
    "\n",
    "data.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "val_wide = process_test_naives_bayes(data=data, metadata=meta, session_length=config['session_length'], encode = True,encoders=encoders,cols_to_encode=list(encoders.keys()))\n",
    "val_wide=val_wide.loc[val_wide.impressions!=0]\n",
    "\n",
    "\n",
    "try:\n",
    "    del test\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    gc.collect()\n",
    "\n",
    "if config['drop_no_references']:\n",
    "    val_wide_not_allnull=val_wide.loc[(val_wide.iloc[:,0:(2*config['session_length']-1)].T != 0).all()].copy()\n",
    "else:\n",
    "    val_wide_not_allnull=val_wide.copy()\n",
    "\n",
    "val_wide_not_allnull['impressions']=val_wide_not_allnull['impressions'].str.split('\\\\|')\n",
    "\n",
    "try:\n",
    "    del val_wide\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_type|1</th>\n",
       "      <th>reference|1</th>\n",
       "      <th>1 Star</th>\n",
       "      <th>2 Star</th>\n",
       "      <th>3 Star</th>\n",
       "      <th>4 Star</th>\n",
       "      <th>5 Star</th>\n",
       "      <th>Accessible Hotel</th>\n",
       "      <th>Accessible Parking</th>\n",
       "      <th>Adults Only</th>\n",
       "      <th>...</th>\n",
       "      <th>Water Slide</th>\n",
       "      <th>Wheelchair Accessible</th>\n",
       "      <th>WiFi (Public Areas)</th>\n",
       "      <th>WiFi (Rooms)</th>\n",
       "      <th>platform</th>\n",
       "      <th>city</th>\n",
       "      <th>device</th>\n",
       "      <th>impressions</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00EI1R7YK601_9639ee039c1d0_3</th>\n",
       "      <td>1</td>\n",
       "      <td>135917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "      <td>3700</td>\n",
       "      <td>0</td>\n",
       "      <td>[135917, 104177, 106691, 1409858, 4529846, 382...</td>\n",
       "      <td>2018-11-01 10:32:09</td>\n",
       "      <td>135917.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00GKOZLYVI9R_8e74b912cb1b4_16</th>\n",
       "      <td>1</td>\n",
       "      <td>5658130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29</td>\n",
       "      <td>8600</td>\n",
       "      <td>1</td>\n",
       "      <td>[3060180, 3176094, 5658130, 4467826, 5200924, ...</td>\n",
       "      <td>2018-11-01 09:41:50</td>\n",
       "      <td>3060180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00J9RN4XAC2N_d2397c03bc9b4_122</th>\n",
       "      <td>1</td>\n",
       "      <td>2714672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "      <td>2111</td>\n",
       "      <td>0</td>\n",
       "      <td>[3134112, 1104106, 2714672, 4073430, 3827338, ...</td>\n",
       "      <td>2018-11-01 09:37:30</td>\n",
       "      <td>3134112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00QD3TS82ZP1_65f6d52da6c28_3</th>\n",
       "      <td>4</td>\n",
       "      <td>3538112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29</td>\n",
       "      <td>5795</td>\n",
       "      <td>0</td>\n",
       "      <td>[3538112, 1518663, 1749805, 3537348, 2230166, ...</td>\n",
       "      <td>2018-11-01 09:26:59</td>\n",
       "      <td>3538112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00VFIQZH8RZ6_b485e039d93c7_33</th>\n",
       "      <td>3</td>\n",
       "      <td>3213142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19</td>\n",
       "      <td>310</td>\n",
       "      <td>1</td>\n",
       "      <td>[6546628, 147278, 3905094, 3087002, 8980586, 1...</td>\n",
       "      <td>2018-11-01 08:54:20</td>\n",
       "      <td>5627048.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                action_type|1 reference|1  1 Star  2 Star  \\\n",
       "key                                                                         \n",
       "00EI1R7YK601_9639ee039c1d0_3                1      135917     0.0     0.0   \n",
       "00GKOZLYVI9R_8e74b912cb1b4_16               1     5658130     0.0     1.0   \n",
       "00J9RN4XAC2N_d2397c03bc9b4_122              1     2714672     0.0     0.0   \n",
       "00QD3TS82ZP1_65f6d52da6c28_3                4     3538112     0.0     0.0   \n",
       "00VFIQZH8RZ6_b485e039d93c7_33               3     3213142     0.0     0.0   \n",
       "\n",
       "                                3 Star  4 Star  5 Star  Accessible Hotel  \\\n",
       "key                                                                        \n",
       "00EI1R7YK601_9639ee039c1d0_3       0.0     1.0     0.0               1.0   \n",
       "00GKOZLYVI9R_8e74b912cb1b4_16      0.0     0.0     0.0               0.0   \n",
       "00J9RN4XAC2N_d2397c03bc9b4_122     0.0     1.0     0.0               0.0   \n",
       "00QD3TS82ZP1_65f6d52da6c28_3       0.0     0.0     0.0               0.0   \n",
       "00VFIQZH8RZ6_b485e039d93c7_33      0.0     0.0     0.0               0.0   \n",
       "\n",
       "                                Accessible Parking  Adults Only    ...      \\\n",
       "key                                                                ...       \n",
       "00EI1R7YK601_9639ee039c1d0_3                   0.0          0.0    ...       \n",
       "00GKOZLYVI9R_8e74b912cb1b4_16                  0.0          0.0    ...       \n",
       "00J9RN4XAC2N_d2397c03bc9b4_122                 0.0          0.0    ...       \n",
       "00QD3TS82ZP1_65f6d52da6c28_3                   0.0          0.0    ...       \n",
       "00VFIQZH8RZ6_b485e039d93c7_33                  0.0          0.0    ...       \n",
       "\n",
       "                                Water Slide  Wheelchair Accessible  \\\n",
       "key                                                                  \n",
       "00EI1R7YK601_9639ee039c1d0_3            0.0                    1.0   \n",
       "00GKOZLYVI9R_8e74b912cb1b4_16           0.0                    0.0   \n",
       "00J9RN4XAC2N_d2397c03bc9b4_122          0.0                    1.0   \n",
       "00QD3TS82ZP1_65f6d52da6c28_3            0.0                    1.0   \n",
       "00VFIQZH8RZ6_b485e039d93c7_33           0.0                    0.0   \n",
       "\n",
       "                                WiFi (Public Areas)  WiFi (Rooms)  platform  \\\n",
       "key                                                                           \n",
       "00EI1R7YK601_9639ee039c1d0_3                    1.0           1.0        32   \n",
       "00GKOZLYVI9R_8e74b912cb1b4_16                   1.0           0.0        29   \n",
       "00J9RN4XAC2N_d2397c03bc9b4_122                  1.0           1.0        27   \n",
       "00QD3TS82ZP1_65f6d52da6c28_3                    0.0           0.0        29   \n",
       "00VFIQZH8RZ6_b485e039d93c7_33                   1.0           1.0        19   \n",
       "\n",
       "                                city  device  \\\n",
       "key                                            \n",
       "00EI1R7YK601_9639ee039c1d0_3    3700       0   \n",
       "00GKOZLYVI9R_8e74b912cb1b4_16   8600       1   \n",
       "00J9RN4XAC2N_d2397c03bc9b4_122  2111       0   \n",
       "00QD3TS82ZP1_65f6d52da6c28_3    5795       0   \n",
       "00VFIQZH8RZ6_b485e039d93c7_33    310       1   \n",
       "\n",
       "                                                                      impressions  \\\n",
       "key                                                                                 \n",
       "00EI1R7YK601_9639ee039c1d0_3    [135917, 104177, 106691, 1409858, 4529846, 382...   \n",
       "00GKOZLYVI9R_8e74b912cb1b4_16   [3060180, 3176094, 5658130, 4467826, 5200924, ...   \n",
       "00J9RN4XAC2N_d2397c03bc9b4_122  [3134112, 1104106, 2714672, 4073430, 3827338, ...   \n",
       "00QD3TS82ZP1_65f6d52da6c28_3    [3538112, 1518663, 1749805, 3537348, 2230166, ...   \n",
       "00VFIQZH8RZ6_b485e039d93c7_33   [6546628, 147278, 3905094, 3087002, 8980586, 1...   \n",
       "\n",
       "                                         timestamp     target  \n",
       "key                                                            \n",
       "00EI1R7YK601_9639ee039c1d0_3   2018-11-01 10:32:09   135917.0  \n",
       "00GKOZLYVI9R_8e74b912cb1b4_16  2018-11-01 09:41:50  3060180.0  \n",
       "00J9RN4XAC2N_d2397c03bc9b4_122 2018-11-01 09:37:30  3134112.0  \n",
       "00QD3TS82ZP1_65f6d52da6c28_3   2018-11-01 09:26:59  3538112.0  \n",
       "00VFIQZH8RZ6_b485e039d93c7_33  2018-11-01 08:54:20  5627048.0  \n",
       "\n",
       "[5 rows x 165 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_wide_not_allnull.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create long table for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imp=val_wide_not_allnull[['impressions','target']].reset_index().copy()\n",
    "\n",
    "val_imp_long=val_imp.impressions.apply(pd.Series) \\\n",
    "    .merge(val_imp, right_index = True, left_index = True) \\\n",
    "    .drop([\"impressions\"], axis = 1)  \\\n",
    "    .melt(id_vars = ['key','target'], value_name = \"impressions\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna() \\\n",
    "    .sort_values('key') \\\n",
    "    .reset_index(drop=True) \\\n",
    "    .copy()\n",
    "\n",
    "try:\n",
    "    del val_imp\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get row, col indexes for extracting probabilities from pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index=val_imp_long['key'].copy()\n",
    "col_index=val_imp_long['impressions'].copy()\n",
    "\n",
    "mask_in_class = col_index.isin(clf.classes_) #True is in class\n",
    "col_index.loc[~mask_in_class]='not_in_class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do predict and munge output on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6549, 165)\n"
     ]
    }
   ],
   "source": [
    "print(val_wide_not_allnull.shape)\n",
    "\n",
    "n_splits = round(val_wide_not_allnull.shape[0]/config['parts_nrows_test'])\n",
    "index_split = round(val_wide_not_allnull.shape[0]/n_splits)\n",
    "\n",
    "impression_probs=([])\n",
    "for i in range(n_splits): # n_splits\n",
    "    if (i+1)==n_splits:\n",
    "        pred=clf.predict_proba(val_wide_not_allnull.iloc[(index_split*(i)):,:]. \\\n",
    "                                         drop(np.append(cols_to_drop,['timestamp', 'impressions','target']),axis=1))\n",
    "        \n",
    "        preddf=pd.DataFrame(data=pred,columns=clf.classes_,index=val_wide_not_allnull.iloc[(index_split*(i)):,:].index)\n",
    "        preddf['not_in_class']=0 # need to return 0 in case impression not in clf.classes_\n",
    "        \n",
    "        first_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):,:].head(1).index\n",
    "        first_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(first_key_last_pred)].index.min()\n",
    "\n",
    "        last_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):,:].tail(1).index\n",
    "        last_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(last_key_last_pred)].index.max()\n",
    "\n",
    "        row_index[first_index_for_row_col:(last_index_for_row_col+1)]\n",
    "        \n",
    "        impression_probs=np.append(impression_probs,preddf.lookup(row_index[first_index_for_row_col:(last_index_for_row_col+1)], \\\n",
    "                                              col_index[first_index_for_row_col:(last_index_for_row_col+1)]),axis=0)\n",
    "        \n",
    "    else:\n",
    "        pred=clf.predict_proba(val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:]. \\\n",
    "                                         drop(np.append(cols_to_drop,['timestamp', 'impressions','target']),axis=1))\n",
    "        \n",
    "        \n",
    "        preddf=pd.DataFrame(data=pred,columns=clf.classes_,index=val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:].index)\n",
    "        preddf['not_in_class']=0 # need to return 0 in case impression not in clf.classes_\n",
    "        \n",
    "        first_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:].head(1).index\n",
    "        first_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(first_key_last_pred)].index.min()\n",
    "\n",
    "        last_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:].tail(1).index\n",
    "        last_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(last_key_last_pred)].index.max()\n",
    "\n",
    "        row_index[first_index_for_row_col:(last_index_for_row_col+1)]\n",
    "        \n",
    "        impression_probs=np.append(impression_probs,preddf.lookup(row_index[first_index_for_row_col:(last_index_for_row_col+1)], \\\n",
    "                                              col_index[first_index_for_row_col:(last_index_for_row_col+1)]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Munge to get rank and RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imp_long['probs'] = impression_probs\n",
    "\n",
    "val_imp_long.sort_values(['key','probs'],ascending=[True,False],inplace=True)\n",
    "\n",
    "val_imp_long['rank'] = 1\n",
    "val_imp_long['rank'] = val_imp_long.groupby('key')['rank'].cumsum()\n",
    "\n",
    "val_imp_long['RR']=1/val_imp_long['rank']\n",
    "\n",
    "val_imp_long.to_csv(config['val_long_csv_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>step</th>\n",
       "      <th>item_recommendations</th>\n",
       "      <th>item_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00EI1R7YK601</td>\n",
       "      <td>9639ee039c1d0</td>\n",
       "      <td>1541068329</td>\n",
       "      <td>3</td>\n",
       "      <td>104136 104132 4529846 393751 3143230 6003158 1...</td>\n",
       "      <td>2.8549866195958558e-05 6.137422847746534e-08 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00GKOZLYVI9R</td>\n",
       "      <td>8e74b912cb1b4</td>\n",
       "      <td>1541065310</td>\n",
       "      <td>16</td>\n",
       "      <td>4467826 5452024 1031940 2596290 8162422 582721...</td>\n",
       "      <td>0.09955310213551848 3.8340571319871677e-05 2.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00J9RN4XAC2N</td>\n",
       "      <td>d2397c03bc9b4</td>\n",
       "      <td>1541065050</td>\n",
       "      <td>122</td>\n",
       "      <td>3134112 1104106 1104108 4073430 4702338 585638...</td>\n",
       "      <td>2.1746880231882764e-07 9.820942034302735e-12 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00QD3TS82ZP1</td>\n",
       "      <td>65f6d52da6c28</td>\n",
       "      <td>1541064419</td>\n",
       "      <td>3</td>\n",
       "      <td>3537348 3536828 2034239 1518663 2832878 390219...</td>\n",
       "      <td>0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00VFIQZH8RZ6</td>\n",
       "      <td>b485e039d93c7</td>\n",
       "      <td>1541062460</td>\n",
       "      <td>33</td>\n",
       "      <td>2005355 7793494 2772604 3213142 100137 9549282...</td>\n",
       "      <td>2.9137703499490515e-12 0.0 0.0 0.0 0.0 0.0 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id     session_id   timestamp step  \\\n",
       "0  00EI1R7YK601  9639ee039c1d0  1541068329    3   \n",
       "1  00GKOZLYVI9R  8e74b912cb1b4  1541065310   16   \n",
       "2  00J9RN4XAC2N  d2397c03bc9b4  1541065050  122   \n",
       "3  00QD3TS82ZP1  65f6d52da6c28  1541064419    3   \n",
       "4  00VFIQZH8RZ6  b485e039d93c7  1541062460   33   \n",
       "\n",
       "                                item_recommendations  \\\n",
       "0  104136 104132 4529846 393751 3143230 6003158 1...   \n",
       "1  4467826 5452024 1031940 2596290 8162422 582721...   \n",
       "2  3134112 1104106 1104108 4073430 4702338 585638...   \n",
       "3  3537348 3536828 2034239 1518663 2832878 390219...   \n",
       "4  2005355 7793494 2772604 3213142 100137 9549282...   \n",
       "\n",
       "                                          item_probs  \n",
       "0  2.8549866195958558e-05 6.137422847746534e-08 9...  \n",
       "1  0.09955310213551848 3.8340571319871677e-05 2.6...  \n",
       "2  2.1746880231882764e-07 9.820942034302735e-12 7...  \n",
       "3        0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  \n",
       "4  2.9137703499490515e-12 0.0 0.0 0.0 0.0 0.0 0.0...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_imp_wide=val_imp_long.pivot(index='key',columns='rank',values=['impressions','probs']).copy()\n",
    "\n",
    "# collapse column multi index for ease of indexing\n",
    "rank_imp_wide.columns=rank_imp_wide.columns.map(lambda x: '|'.join([str(i) for i in x]))\n",
    "    \n",
    "rank_imp_wide=rank_imp_wide.join(val_wide_not_allnull['timestamp'], on='key')\n",
    "\n",
    "rank_imp_wide['timestamp']=rank_imp_wide['timestamp'].astype(np.int64)//10**9\n",
    "\n",
    "rank_imp_wide.reset_index(inplace=True)\n",
    "\n",
    "rank_imp_wide[['user_id','session_id','step']]=rank_imp_wide['key'].str.split('_',expand=True)\n",
    "\n",
    "rank_imp_wide.drop(['key'],axis=1,inplace=True)\n",
    "\n",
    "rank_imp_wide['item_recommendations'] = rank_imp_wide.iloc[:,:25].apply(\n",
    "    lambda x: ' '.join(x.dropna()),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "rank_imp_wide.drop(rank_imp_wide.columns[:25],axis=1,inplace=True)\n",
    "\n",
    "rank_imp_wide['item_probs'] = rank_imp_wide.iloc[:,:25].apply(\n",
    "    lambda x: ' '.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "rank_imp_wide.drop(rank_imp_wide.columns[:25],axis=1,inplace=True)\n",
    "\n",
    "cols=rank_imp_wide.columns.tolist()\n",
    "cols=cols[1:3]+cols[:1]+cols[3:4]+cols[-2:]\n",
    "rank_imp_wide=rank_imp_wide[cols]\n",
    "\n",
    "rank_imp_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_imp_wide.drop('item_probs',axis=1).to_csv(config['output_recsys_csv_path'],index=False)\n",
    "rank_imp_wide.to_csv(config['output_meta_csv_path'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.593750782743839\n",
      "0.2656982324738837\n",
      "support MRR_NB:  1936\n",
      "support MRR_NB:  6544\n"
     ]
    }
   ],
   "source": [
    "if config['use_validation']:\n",
    "    MRR_NB=val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)) \\\n",
    "                           & (val_imp_long.probs!=0.0),'RR'].mean()\n",
    "    MRR_parse_imp=val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)),'RR'].mean()\n",
    "    \n",
    "    print(MRR_NB)\n",
    "    print(MRR_parse_imp)\n",
    "    print('support MRR_NB: ', str(val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)) & (val_imp_long.probs!=0.0),'RR'].count()))\n",
    "    print('support MRR_NB: ', str(val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)),'RR'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1936'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)) & (val_imp_long.probs!=0.0),'RR'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add lost sessions back"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
