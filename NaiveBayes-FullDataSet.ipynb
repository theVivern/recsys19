{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import time\n",
    "\n",
    "# %% Imports\n",
    "from pathlib import Path\n",
    "import sys\n",
    "root_dir = Path().resolve()\n",
    "sys.path.append(str(root_dir / 'src'))\n",
    "\n",
    "from recsys_common import *\n",
    "from recsys_naive_bayes_processing import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "config= {\n",
    "    'save_train_test_val': True,\n",
    "    'load_fitted_model': False,\n",
    "    \n",
    "    'use_subset': True,\n",
    "    'subset_frac': 0.05,\n",
    "    'use_validation': True,\n",
    "    'validation_frac': 0.25,\n",
    "    'reference_to_nan_frac': 1,\n",
    "    'reference_to_nan_seed': 1234,\n",
    "    \n",
    "    'session_length': 1,\n",
    "    'drop_no_references': True,\n",
    "    'train_model_on_test_data': True,\n",
    "    'add_prices': False,\n",
    "    'add_hour': False,\n",
    "    'cols_to_append': [],#['platform','city','device'],\n",
    "    'drop_action_type': True,\n",
    "    \n",
    "    'train_session_chunksize': 15000,\n",
    "    'parts_nrows_test': 10000,\n",
    "    'data_path': root_dir / 'cache'\n",
    "    }\n",
    "\n",
    "\n",
    "if not config['use_subset']:\n",
    "    config['subset_frac']=1\n",
    "\n",
    "config['le_pickle_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) + '_le.pickle')\n",
    "config['train_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_train.csv')\n",
    "config['train_last_step_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_train_last_step.csv')\n",
    "config['test_last_step_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_test_last_step.csv')\n",
    "config['test_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_test.csv')\n",
    "config['val_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_val.csv')\n",
    "config['prices_pickle_path']=config['data_path'] / 'mean_prices.pickle'\n",
    "config['model_pickle_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_model.pickle')\n",
    "config['val_long_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_val_long.csv')\n",
    "config['output_recsys_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_output_recsys.csv')\n",
    "config['output_meta_csv_path']=config['data_path'] / ('NB_data_' + str(int(100*config['subset_frac'])).zfill(3) + '_' + str(config['session_length']) +  '_output_meta.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['session_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta=get_metadata()\n",
    "# meta.dtypes\n",
    "\n",
    "meta['item_id']=meta['item_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sessions\n",
      "Filter session with no clickout\n",
      "user_id                         object\n",
      "session_id                      object\n",
      "timestamp          datetime64[ns, UTC]\n",
      "step                             int64\n",
      "action_type                     object\n",
      "reference                       object\n",
      "platform                        object\n",
      "city                            object\n",
      "device                          object\n",
      "current_filters                 object\n",
      "impressions                     object\n",
      "prices                          object\n",
      "is_validation                     bool\n",
      "is_train                          bool\n",
      "target                          object\n",
      "dtype: object\n",
      "Quick unit test\n",
      "893499\n",
      "893499\n",
      "10336\n",
      "0\n",
      "0\n",
      "19995\n",
      "10336\n",
      "59661\n",
      "0\n",
      "26345\n",
      "12856\n",
      "Train encoders and save\n",
      "['change of sort order' 'clickout item' 'filter selection'\n",
      " 'interaction item deals' 'interaction item image' 'interaction item info'\n",
      " 'interaction item rating' 'search for destination' 'search for item'\n",
      " 'search for poi']\n",
      "['AA' 'AE' 'AR' 'AT' 'AU' 'BE' 'BG' 'BR' 'CA' 'CH' 'CL' 'CN' 'CO' 'CZ'\n",
      " 'DE' 'DK' 'EC' 'ES' 'FI' 'FR' 'GR' 'HK' 'HR' 'HU' 'ID' 'IE' 'IL' 'IN'\n",
      " 'IT' 'JP' 'KR' 'MX' 'MY' 'NL' 'NO' 'NZ' 'PE' 'PH' 'PL' 'PT' 'RO' 'RS'\n",
      " 'RU' 'SE' 'SG' 'SI' 'SK' 'TH' 'TR' 'TW' 'UK' 'US' 'UY' 'VN' 'ZA']\n",
      "['A Teixeira, Spain' 'Aachen, Germany' 'Aadorf, Switzerland' ...\n",
      " 'Żarki, Poland' 'Žabljak, Montenegro' 'Žilina, Slovakia']\n",
      "['desktop' 'mobile' 'tablet']\n",
      "Get Splits\n",
      "train (537963, 10)\n",
      "test (200069, 13)\n",
      "val (155467, 13)\n",
      "train user_id                         object\n",
      "session_id                      object\n",
      "timestamp          datetime64[ns, UTC]\n",
      "step                             int64\n",
      "action_type                     object\n",
      "reference                       object\n",
      "platform                        object\n",
      "city                            object\n",
      "device                          object\n",
      "current_filters                 object\n",
      "dtype: object\n",
      "test user_id                         object\n",
      "session_id                      object\n",
      "timestamp          datetime64[ns, UTC]\n",
      "step                             int64\n",
      "action_type                     object\n",
      "reference                       object\n",
      "platform                        object\n",
      "city                            object\n",
      "device                          object\n",
      "current_filters                 object\n",
      "impressions                     object\n",
      "prices                          object\n",
      "target                          object\n",
      "dtype: object\n",
      "val user_id                         object\n",
      "session_id                      object\n",
      "timestamp          datetime64[ns, UTC]\n",
      "step                             int64\n",
      "action_type                     object\n",
      "reference                       object\n",
      "platform                        object\n",
      "city                            object\n",
      "device                          object\n",
      "current_filters                 object\n",
      "impressions                     object\n",
      "prices                          object\n",
      "target                          object\n",
      "dtype: object\n",
      "Save either test or val\n",
      "delete session, test and val\n",
      "save train\n",
      "delete train\n"
     ]
    }
   ],
   "source": [
    "if config['save_train_test_val']:\n",
    "    print('Getting sessions')\n",
    "    sessions=get_sessions(config['use_subset'],\n",
    "                          config['subset_frac'],\n",
    "                          config['use_validation'],\n",
    "                          config['validation_frac'],\n",
    "                          config['reference_to_nan_frac'],\n",
    "                          config['reference_to_nan_seed'])\n",
    "\n",
    "    print('Filter session with no clickout')\n",
    "    if (not config['use_validation']) & (not config['use_subset']):\n",
    "        print('filtering sessions with clickout')\n",
    "        sessions=filter_sessions_with_no_clicks(sessions)\n",
    "\n",
    "        \n",
    "    columns_to_encode = ['action_type','platform','city','device']\n",
    "\n",
    "    if config['add_prices']:\n",
    "        print('adding prices')\n",
    "        prices=pd.read_pickle(config['prices_pickle_path'])\n",
    "        sessions=sessions.join(prices.set_index('reference'),on='reference')\n",
    "#         sessions['city_price']=sessions['city'] + sessions['mean_prices'].astype(str)\n",
    "#         columns_to_encode=np.append(columns_to_encode,['city_price'])\n",
    "    \n",
    "    if config['add_hour']:\n",
    "        sessions['hour']=sessions.timestamp.dt.hour.astype(str)\n",
    "        sessions['city_hour']=sessions['city'] + sessions['hour']\n",
    "        sessions.drop('hour',axis=1,inplace=True)\n",
    "        columns_to_encode=np.append(columns_to_encode,['city_hour'])\n",
    "        config['cols_to_append']=np.append(config['cols_to_append'],['city_hour'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(sessions.dtypes)\n",
    "    # sessions.head()\n",
    "\n",
    "    print('Quick unit test')\n",
    "\n",
    "    print(len(sessions.index))\n",
    "    print(len(sessions.index.unique()))\n",
    "\n",
    "    if not config['use_validation']:\n",
    "        sessions['is_validation']=False\n",
    "        sessions['target']=np.NaN\n",
    "\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==True),'target'].count())\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False),'target'].count())\n",
    "    print(sessions.loc[(sessions.is_train==False) & (sessions.is_validation==False),'target'].count())\n",
    "\n",
    "    print(sessions.loc[(sessions.is_validation==True) & (sessions.action_type=='clickout item'),'step'].count())\n",
    "    print(sessions.loc[(sessions.is_validation==True) & (sessions.action_type=='clickout item') & (sessions.reference.isnull()),'step'].count())\n",
    "\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False) & (sessions.action_type=='clickout item') ,'step'].count())\n",
    "    print(sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False) & (sessions.action_type=='clickout item') & (sessions.reference.isnull()),'step'].count())\n",
    "\n",
    "    print(sessions.loc[(sessions.is_train==False) & (sessions.is_validation==False) & (sessions.action_type=='clickout item'),'step'].count())\n",
    "    print(sessions.loc[(sessions.is_train==False) & (sessions.is_validation==False) & (sessions.action_type=='clickout item') & (sessions.reference.isnull()),'step'].count())\n",
    "    \n",
    "    if config['use_validation']:\n",
    "        n_clickouts_test = sessions.loc[(sessions.is_validation==True) & (sessions.action_type=='clickout item') & (sessions.reference.isnull()),'step'].count()\n",
    "    else:\n",
    "        n_clickouts_test = sessions.loc[(sessions.is_train==False) & (sessions.is_validation==False) & (sessions.action_type=='clickout item') & (sessions.reference.isnull()),'step'].count()\n",
    "        \n",
    "        \n",
    "    # Get possible classes\n",
    "    # classes=list(set(train.loc[(train.action_type=='clickout item') & (train.step>1),'reference']))\n",
    "    classes=list(set(sessions.loc[(sessions.action_type=='clickout item') & ~(sessions.reference.isnull()),'reference']))\n",
    "\n",
    "    \n",
    "    print('Train encoders and save')\n",
    "    encoders = {}\n",
    "    for col in columns_to_encode:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        encoders[col]=le.fit(sessions[col])\n",
    "        print(encoders[col].classes_)\n",
    "    #     val_wide[col]=encoders[col].transform(val_wide[col])\n",
    "\n",
    "    with open(config['le_pickle_path'], 'wb') as handle:\n",
    "        pickle.dump(encoders, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # with open(config['le_pickle_path'], 'rb') as handle:\n",
    "    #     b = pickle.load(handle)\n",
    "    \n",
    "    print('Get Splits')\n",
    "\n",
    "    train = sessions.loc[(sessions.is_train==True) & (sessions.is_validation==False)] \\\n",
    "                        .drop(['impressions','prices','is_train','is_validation','target'],axis=1) \\\n",
    "                        .reset_index(drop=True)\n",
    "\n",
    "    test = sessions.loc[sessions.is_train==False] \\\n",
    "                       .drop(['is_train','is_validation'],axis=1) \\\n",
    "                       .reset_index(drop=True)\n",
    "\n",
    "    val = sessions.loc[(sessions.is_train==True) & (sessions.is_validation==True)] \\\n",
    "                       .drop(['is_train','is_validation'],axis=1) \\\n",
    "                       .reset_index(drop=True)\n",
    "\n",
    "    print('train',train.shape)\n",
    "    print('test',test.shape)\n",
    "    print('val',val.shape)\n",
    "\n",
    "    print('train',train.dtypes)\n",
    "    print('test',test.dtypes)\n",
    "    print('val',val.dtypes)\n",
    "\n",
    "\n",
    "    print('Save either test or val')\n",
    "    if config['use_validation']:\n",
    "        val.to_csv(config['val_csv_path'])\n",
    "    else:\n",
    "        test.to_csv(config['test_csv_path'])\n",
    "\n",
    "\n",
    "    print('delete session, test and val')\n",
    "    try:\n",
    "        del sessions, test, val\n",
    "    except NameError:\n",
    "        pass\n",
    "    else:\n",
    "        gc.collect()\n",
    "     \n",
    "    \n",
    "\n",
    "\n",
    "    print('save train')\n",
    "    train.to_csv(config['train_csv_path'])\n",
    "\n",
    "else:\n",
    "    print('loading train and encoders...')\n",
    "    \n",
    "    train=pd.read_csv(config['train_csv_path'])\n",
    "    train.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "    \n",
    "    with open(config['le_pickle_path'], 'rb') as handle:\n",
    "        encoders = pickle.load(handle)\n",
    "\n",
    "    print('done')\n",
    "    \n",
    "\n",
    "last_step_per_session=train.groupby('session_id',sort=False)['step'].max().reset_index().to_csv(config['train_last_step_csv_path'])\n",
    "\n",
    "names=train.columns\n",
    "\n",
    "\n",
    "print('delete train')\n",
    "try:\n",
    "    del train\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference|1\n",
      "action_type|1\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop=([])\n",
    "for i in range(config['session_length']):\n",
    "    print('reference|' + str(i+1))\n",
    "    cols_to_drop=np.append(cols_to_drop,'reference|' + str(i+1))\n",
    "    if config['drop_action_type']:\n",
    "        print('action_type|' + str(i+1))\n",
    "        cols_to_drop=np.append(cols_to_drop,'action_type|' + str(i+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Train & process\n",
      "(19745, 160)\n",
      "Get Train & process\n",
      "(20234, 160)\n",
      "Get Train & process\n",
      "(1396, 160)\n",
      "Seconds elapsed  : 249.72301602363586\n"
     ]
    }
   ],
   "source": [
    "if not config['load_fitted_model']:\n",
    "    start = time.time()\n",
    "    reader=pd.read_csv(config['train_last_step_csv_path'], chunksize=config['train_session_chunksize'])\n",
    "\n",
    "    skiprows=0\n",
    "    clf=BernoulliNB()\n",
    "    for i,chunk in enumerate(reader):\n",
    "        nrows=chunk.step.sum()\n",
    "        print('Get Train & process')\n",
    "        train_part=pd.read_csv(config['train_csv_path'],header=0,skiprows=skiprows,nrows=nrows,names=names)\n",
    "\n",
    "        train_wide = process_train_naives_bayes(data=train_part, metadata=meta, encoders=encoders, config=config)\n",
    "\n",
    "        del train_part\n",
    "\n",
    "        if config['drop_no_references']:\n",
    "            train_wide_not_allnull=train_wide[(train_wide.iloc[:,0:((2*config['session_length'])-1)].T != 0).any()].copy()\n",
    "        else:\n",
    "            train_wide_not_allnull=train_wide.copy()\n",
    "\n",
    "\n",
    "#         del train_wide\n",
    "\n",
    "        print(train_wide_not_allnull.shape)\n",
    "        \n",
    "        if not train_wide_not_allnull.empty:\n",
    "            if i==0:\n",
    "                clf.partial_fit(train_wide_not_allnull \\\n",
    "                            .drop(np.append(cols_to_drop,'y'),axis=1), \\\n",
    "                                   np.ravel(train_wide_not_allnull.y), \\\n",
    "                                   classes = classes)\n",
    "            else:\n",
    "                clf.partial_fit(train_wide_not_allnull \\\n",
    "                            .drop(np.append(cols_to_drop,'y'),axis=1), \\\n",
    "                                   np.ravel(train_wide_not_allnull.y))\n",
    "\n",
    "        skiprows += nrows\n",
    "\n",
    "    stop = time.time()\n",
    "    print('Seconds elapsed  :', stop-start) \n",
    "    \n",
    "    # Output a pickle file for the model\n",
    "    joblib.dump(clf, config['model_pickle_path'])\n",
    "    \n",
    "    del train_wide_not_allnull\n",
    "    \n",
    "else:\n",
    "    clf=joblib.load(config['model_pickle_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on test data clickouts that have a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7168, 160)\n",
      "Seconds elapsed  : 20.790571212768555\n"
     ]
    }
   ],
   "source": [
    "if config['train_model_on_test_data']:\n",
    "    \n",
    "    if config['use_validation']:\n",
    "        data=pd.read_csv(config['val_csv_path'],parse_dates=['timestamp'])\n",
    "        test_path=config['val_csv_path']\n",
    "    else:\n",
    "        data=pd.read_csv(config['test_csv_path'],parse_dates=['timestamp'])\n",
    "        test_path=config['test_csv_path']\n",
    "        \n",
    "    names2=data.columns\n",
    "        \n",
    "    last_step_per_session=data.groupby('session_id',sort=False)['step'].max().reset_index().to_csv(config['test_last_step_csv_path'])\n",
    "\n",
    "    start = time.time()\n",
    "    reader=pd.read_csv(config['test_last_step_csv_path'], chunksize=config['train_session_chunksize'])\n",
    "\n",
    "    skiprows=0\n",
    "    \n",
    "    for i,chunk in enumerate(reader):\n",
    "        nrows=chunk.step.sum()\n",
    "        train_part=pd.read_csv(test_path,header=0,skiprows=skiprows,nrows=nrows,names=names2)\n",
    "        \n",
    "        train_part=train_part.loc[~(train_part.reference.isnull())]\n",
    "        train_part.drop(['impressions','prices','target'],axis=1,inplace=True)\n",
    "\n",
    "        train_wide = process_train_naives_bayes(data=train_part, metadata=meta, encoders=encoders, config=config)\n",
    "\n",
    "        del train_part\n",
    "\n",
    "        if config['drop_no_references']:\n",
    "            train_wide_not_allnull=train_wide[(train_wide.iloc[:,0:((2*config['session_length'])-1)].T != 0).any()].copy()\n",
    "        else:\n",
    "            train_wide_not_allnull=train_wide.copy()\n",
    "\n",
    "\n",
    "        del train_wide\n",
    "\n",
    "        print(train_wide_not_allnull.shape)\n",
    "        \n",
    "        if not train_wide_not_allnull.empty:\n",
    "            clf.partial_fit(train_wide_not_allnull \\\n",
    "                        .drop(np.append(cols_to_drop,'y'),axis=1), \\\n",
    "                               np.ravel(train_wide_not_allnull.y))\n",
    "\n",
    "        skiprows += nrows\n",
    "\n",
    "    stop = time.time()\n",
    "    print('Seconds elapsed  :', stop-start) \n",
    "    \n",
    "    # Output a pickle file for the model\n",
    "    joblib.dump(clf, config['model_pickle_path'])\n",
    "    \n",
    "#     del train_wide_not_allnull\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get test data in wide format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split for partial fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['use_validation']:\n",
    "    data=pd.read_csv(config['val_csv_path'],parse_dates=['timestamp'])\n",
    "else:\n",
    "    data=pd.read_csv(config['test_csv_path'],parse_dates=['timestamp'])\n",
    "\n",
    "data.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "val_wide = process_test_naives_bayes(data=data, metadata=meta, encoders=encoders, config=config)\n",
    "val_wide=val_wide.loc[val_wide.impressions!=0]\n",
    "\n",
    "\n",
    "try:\n",
    "    del test\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    gc.collect()\n",
    "\n",
    "if config['drop_no_references']:\n",
    "    val_wide_not_allnull=val_wide.loc[(val_wide.iloc[:,0:((2*config['session_length'])-1)].T != 0).any()].copy()\n",
    "else:\n",
    "    val_wide_not_allnull=val_wide.copy()\n",
    "\n",
    "val_wide_not_allnull['impressions']=val_wide_not_allnull['impressions'].str.split('\\\\|')\n",
    "\n",
    "try:\n",
    "    del val_wide\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create long table for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imp=val_wide_not_allnull[['impressions','target']].reset_index().copy()\n",
    "\n",
    "val_imp_long=val_imp.impressions.apply(pd.Series) \\\n",
    "    .merge(val_imp, right_index = True, left_index = True) \\\n",
    "    .drop([\"impressions\"], axis = 1)  \\\n",
    "    .melt(id_vars = ['key','target'], value_name = \"impressions\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna() \\\n",
    "    .sort_values('key') \\\n",
    "    .reset_index(drop=True) \\\n",
    "    .copy()\n",
    "\n",
    "try:\n",
    "    del val_imp\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get row, col indexes for extracting probabilities from pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index=val_imp_long['key'].copy()\n",
    "col_index=val_imp_long['impressions'].copy()\n",
    "\n",
    "mask_in_class = col_index.isin(clf.classes_) #True is in class\n",
    "col_index.loc[~mask_in_class]='not_in_class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do predict and munge output on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6549, 162)\n",
      "6549\n",
      "0 of 1\n"
     ]
    }
   ],
   "source": [
    "print(val_wide_not_allnull.shape)\n",
    "\n",
    "n_splits = round(val_wide_not_allnull.shape[0]/config['parts_nrows_test'])\n",
    "if n_splits==0:\n",
    "    n_splits=1\n",
    "    index_split = val_wide_not_allnull.shape[0]\n",
    "else:\n",
    "    index_split = round(val_wide_not_allnull.shape[0]/n_splits)\n",
    "    \n",
    "print(index_split)\n",
    "\n",
    "impression_probs=([])\n",
    "for i in range(n_splits): # n_splits\n",
    "    print('%s of %s' % (i,n_splits))\n",
    "    if (i+1)==n_splits:\n",
    "        pred=clf.predict_proba(val_wide_not_allnull.iloc[(index_split*(i)):,:] \\\n",
    "                                         .drop(np.append(cols_to_drop,['timestamp', 'impressions','target']),axis=1))\n",
    "        \n",
    "        preddf=pd.DataFrame(data=pred,columns=clf.classes_,index=val_wide_not_allnull.iloc[(index_split*(i)):,:].index)\n",
    "        preddf['not_in_class']=0 # need to return 0 in case impression not in clf.classes_\n",
    "        \n",
    "        first_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):,:].head(1).index\n",
    "        first_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(first_key_last_pred)].index.min()\n",
    "\n",
    "        last_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):,:].tail(1).index\n",
    "        last_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(last_key_last_pred)].index.max()\n",
    "\n",
    "        row_index[first_index_for_row_col:(last_index_for_row_col+1)]\n",
    "        \n",
    "        impression_probs=np.append(impression_probs,preddf.lookup(row_index[first_index_for_row_col:(last_index_for_row_col+1)], \\\n",
    "                                              col_index[first_index_for_row_col:(last_index_for_row_col+1)]),axis=0)\n",
    "        \n",
    "    else:\n",
    "        pred=clf.predict_proba(val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:] \\\n",
    "                                         .drop(np.append(cols_to_drop,['timestamp', 'impressions','target']),axis=1))\n",
    "        \n",
    "        \n",
    "        preddf=pd.DataFrame(data=pred,columns=clf.classes_,index=val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:].index)\n",
    "        preddf['not_in_class']=0 # need to return 0 in case impression not in clf.classes_\n",
    "        \n",
    "        first_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:].head(1).index\n",
    "        first_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(first_key_last_pred)].index.min()\n",
    "\n",
    "        last_key_last_pred=val_wide_not_allnull.iloc[(index_split*(i)):(index_split*(i+1)),:].tail(1).index\n",
    "        last_index_for_row_col=val_imp_long.loc[val_imp_long.key.isin(last_key_last_pred)].index.max()\n",
    "\n",
    "        row_index[first_index_for_row_col:(last_index_for_row_col+1)]\n",
    "        \n",
    "        impression_probs=np.append(impression_probs,preddf.lookup(row_index[first_index_for_row_col:(last_index_for_row_col+1)], \\\n",
    "                                              col_index[first_index_for_row_col:(last_index_for_row_col+1)]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Munge to get rank and RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imp_long['probs'] = impression_probs\n",
    "\n",
    "val_imp_long.sort_values(['key','probs'],ascending=[True,False],inplace=True)\n",
    "\n",
    "val_imp_long['rank'] = 1\n",
    "val_imp_long['rank'] = val_imp_long.groupby('key')['rank'].cumsum()\n",
    "\n",
    "val_imp_long['RR']=1/val_imp_long['rank']\n",
    "\n",
    "val_imp_long.to_csv(config['val_long_csv_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>step</th>\n",
       "      <th>item_recommendations</th>\n",
       "      <th>item_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00EI1R7YK601</td>\n",
       "      <td>9639ee039c1d0</td>\n",
       "      <td>1541068329</td>\n",
       "      <td>3</td>\n",
       "      <td>135917 104136 104132 4529846 104168 393751 314...</td>\n",
       "      <td>2.17473842370666e-06 2.17473842370666e-06 1.91...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00GKOZLYVI9R</td>\n",
       "      <td>8e74b912cb1b4</td>\n",
       "      <td>1541065310</td>\n",
       "      <td>16</td>\n",
       "      <td>4467826 1031940 5452024 2596290 5827212 816242...</td>\n",
       "      <td>0.09313640823079226 0.0002826738846636392 3.23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00J9RN4XAC2N</td>\n",
       "      <td>d2397c03bc9b4</td>\n",
       "      <td>1541065050</td>\n",
       "      <td>122</td>\n",
       "      <td>3134112 1104106 1104108 4073430 2714672 470233...</td>\n",
       "      <td>3.641875313123964e-06 8.947170035210208e-12 2....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00QD3TS82ZP1</td>\n",
       "      <td>65f6d52da6c28</td>\n",
       "      <td>1541064419</td>\n",
       "      <td>3</td>\n",
       "      <td>3537348 3536828 2034239 1518663 2832878 390219...</td>\n",
       "      <td>0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00VFIQZH8RZ6</td>\n",
       "      <td>b485e039d93c7</td>\n",
       "      <td>1541062460</td>\n",
       "      <td>33</td>\n",
       "      <td>3087002 100137 2005355 7793494 2772604 3213142...</td>\n",
       "      <td>5.197289709582199e-07 6.34434779001736e-11 3.9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id     session_id   timestamp step  \\\n",
       "0  00EI1R7YK601  9639ee039c1d0  1541068329    3   \n",
       "1  00GKOZLYVI9R  8e74b912cb1b4  1541065310   16   \n",
       "2  00J9RN4XAC2N  d2397c03bc9b4  1541065050  122   \n",
       "3  00QD3TS82ZP1  65f6d52da6c28  1541064419    3   \n",
       "4  00VFIQZH8RZ6  b485e039d93c7  1541062460   33   \n",
       "\n",
       "                                item_recommendations  \\\n",
       "0  135917 104136 104132 4529846 104168 393751 314...   \n",
       "1  4467826 1031940 5452024 2596290 5827212 816242...   \n",
       "2  3134112 1104106 1104108 4073430 2714672 470233...   \n",
       "3  3537348 3536828 2034239 1518663 2832878 390219...   \n",
       "4  3087002 100137 2005355 7793494 2772604 3213142...   \n",
       "\n",
       "                                          item_probs  \n",
       "0  2.17473842370666e-06 2.17473842370666e-06 1.91...  \n",
       "1  0.09313640823079226 0.0002826738846636392 3.23...  \n",
       "2  3.641875313123964e-06 8.947170035210208e-12 2....  \n",
       "3        0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  \n",
       "4  5.197289709582199e-07 6.34434779001736e-11 3.9...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_imp_wide=val_imp_long.pivot(index='key',columns='rank',values=['impressions','probs']).copy()\n",
    "\n",
    "# collapse column multi index for ease of indexing\n",
    "rank_imp_wide.columns=rank_imp_wide.columns.map(lambda x: '|'.join([str(i) for i in x]))\n",
    "    \n",
    "rank_imp_wide=rank_imp_wide.join(val_wide_not_allnull['timestamp'], on='key')\n",
    "\n",
    "rank_imp_wide['timestamp']=rank_imp_wide['timestamp'].astype(np.int64)//10**9\n",
    "\n",
    "rank_imp_wide.reset_index(inplace=True)\n",
    "\n",
    "rank_imp_wide[['user_id','session_id','step']]=rank_imp_wide['key'].str.split('_',expand=True)\n",
    "\n",
    "rank_imp_wide.drop(['key'],axis=1,inplace=True)\n",
    "\n",
    "rank_imp_wide['item_recommendations'] = rank_imp_wide.iloc[:,:25].apply(\n",
    "    lambda x: ' '.join(x.dropna()),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "rank_imp_wide.drop(rank_imp_wide.columns[:25],axis=1,inplace=True)\n",
    "\n",
    "rank_imp_wide['item_probs'] = rank_imp_wide.iloc[:,:25].apply(\n",
    "    lambda x: ' '.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "rank_imp_wide.drop(rank_imp_wide.columns[:25],axis=1,inplace=True)\n",
    "\n",
    "cols=rank_imp_wide.columns.tolist()\n",
    "cols=cols[1:3]+cols[:1]+cols[3:4]+cols[-2:]\n",
    "rank_imp_wide=rank_imp_wide[cols]\n",
    "\n",
    "rank_imp_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6253893914008776\n",
      "0.3351502456170596\n",
      "support MRR_NB:  2793\n",
      "support MRR_NB:  6544\n",
      "10336\n"
     ]
    }
   ],
   "source": [
    "if config['use_validation']:\n",
    "    MRR_NB=val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)) \\\n",
    "                           & (val_imp_long.probs!=0.0),'RR'].mean()\n",
    "    MRR_parse_imp=val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)),'RR'].mean()\n",
    "    \n",
    "    print(MRR_NB)\n",
    "    print(MRR_parse_imp)\n",
    "    print('support MRR_NB: ', str(val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)) & (val_imp_long.probs!=0.0),'RR'].count()))\n",
    "    print('support MRR_NB: ', str(val_imp_long.loc[(val_imp_long.target.astype(int)==val_imp_long.impressions.astype(int)),'RR'].count()))\n",
    "    print(n_clickouts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6155825128294519\n",
      "1262\n"
     ]
    }
   ],
   "source": [
    "threshold=0.1\n",
    "\n",
    "sum_probs=val_imp_long.groupby('key').probs.sum()\n",
    "val_imp_sum=val_imp_long.join(sum_probs,on='key',rsuffix='_sum')\n",
    "print(val_imp_sum.loc[(val_imp_sum.probs_sum>threshold) & (val_imp_sum.target.astype(int)==val_imp_sum.impressions.astype(int))].RR.mean())\n",
    "print(val_imp_sum.loc[(val_imp_sum.probs_sum>threshold) & (val_imp_sum.target.astype(int)==val_imp_sum.impressions.astype(int))].RR.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add lost sessions back\n",
    "Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['drop_no_references']:\n",
    "    if config['use_validation']:\n",
    "        data=pd.read_csv(config['val_csv_path'],parse_dates=['timestamp'])\n",
    "    else:\n",
    "        data=pd.read_csv(config['test_csv_path'],parse_dates=['timestamp'])\n",
    "\n",
    "    data.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "    val_wide = process_test_naives_bayes(data=data, metadata=meta, encoders=encoders, config=config)\n",
    "    val_wide=val_wide.loc[val_wide.impressions!=0]\n",
    "\n",
    "    try:\n",
    "        del test\n",
    "    except NameError:\n",
    "        pass\n",
    "    else:\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    val_wide_allnull=val_wide.loc[(val_wide.iloc[:,0:(2*config['session_length']-1)].T == 0).all()].copy()\n",
    "    \n",
    "    try:\n",
    "        del val_wide\n",
    "    except NameError:\n",
    "        pass\n",
    "    else:\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Munge into same format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>step</th>\n",
       "      <th>item_recommendations</th>\n",
       "      <th>item_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00M5AMMLYQG5</td>\n",
       "      <td>8fd417ebd2d5b</td>\n",
       "      <td>1541062793</td>\n",
       "      <td>1</td>\n",
       "      <td>9882016 106769 48219 147360 7333050 3533848 75...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00WWQYCIDPUF</td>\n",
       "      <td>a8962895bad41</td>\n",
       "      <td>1541064252</td>\n",
       "      <td>6</td>\n",
       "      <td>7929996 8767518 5742058 7195114 7325018 196079...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0168S4C5E60K</td>\n",
       "      <td>016150a298e6e</td>\n",
       "      <td>1541063179</td>\n",
       "      <td>9</td>\n",
       "      <td>42109 42127 42270 137704 1577271 1750769 42146...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01BS6LPQ5JTX</td>\n",
       "      <td>2a35cec32ede2</td>\n",
       "      <td>1541063117</td>\n",
       "      <td>1</td>\n",
       "      <td>127745 43084 1257131 1797067 1848773 5452918 4...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01MTZ6S2OHUA</td>\n",
       "      <td>2d123642df132</td>\n",
       "      <td>1541059612</td>\n",
       "      <td>10</td>\n",
       "      <td>1277246 1341812 1550767 1388724 2187086 124244...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id     session_id   timestamp step  \\\n",
       "0  00M5AMMLYQG5  8fd417ebd2d5b  1541062793    1   \n",
       "1  00WWQYCIDPUF  a8962895bad41  1541064252    6   \n",
       "2  0168S4C5E60K  016150a298e6e  1541063179    9   \n",
       "3  01BS6LPQ5JTX  2a35cec32ede2  1541063117    1   \n",
       "4  01MTZ6S2OHUA  2d123642df132  1541059612   10   \n",
       "\n",
       "                                item_recommendations  \\\n",
       "0  9882016 106769 48219 147360 7333050 3533848 75...   \n",
       "1  7929996 8767518 5742058 7195114 7325018 196079...   \n",
       "2  42109 42127 42270 137704 1577271 1750769 42146...   \n",
       "3  127745 43084 1257131 1797067 1848773 5452918 4...   \n",
       "4  1277246 1341812 1550767 1388724 2187086 124244...   \n",
       "\n",
       "                                          item_probs  \n",
       "0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "1  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "2       0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0   \n",
       "3  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "4               0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0   "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_ordered = ['user_id', 'session_id', 'timestamp', 'step', 'item_recommendations',  'item_probs']\n",
    "\n",
    "val_wide_allnull=val_wide_allnull[['timestamp','impressions']]\n",
    "val_wide_allnull['timestamp']=val_wide_allnull['timestamp'].astype(np.int64)//10**9\n",
    "val_wide_allnull.reset_index(inplace=True)\n",
    "val_wide_allnull[['user_id','session_id','step']]=val_wide_allnull['key'].str.split('_',expand=True)\n",
    "val_wide_allnull.drop(['key'],axis=1,inplace=True)\n",
    "val_wide_allnull['item_probs']=val_wide_allnull['impressions'].str.replace('[0-9]+\\\\||[0-9]+','0 ')\n",
    "val_wide_allnull['impressions']=val_wide_allnull['impressions'].str.replace('\\\\|',' ')\n",
    "val_wide_allnull.rename(columns={'impressions':'item_recommendations'},inplace=True)\n",
    "\n",
    "\n",
    "columns_ordered = ['user_id', 'session_id', 'timestamp', 'step', 'item_recommendations',  'item_probs']\n",
    "val_wide_allnull=val_wide_allnull.reindex(columns=columns_ordered)\n",
    "\n",
    "val_wide_allnull.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "append with NB fitted test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=rank_imp_wide.append(val_wide_allnull,ignore_index=True,sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.drop('item_probs',axis=1).to_csv(config['output_recsys_csv_path'],index=False)\n",
    "output.to_csv(config['output_meta_csv_path'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10336, 6)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3787, 6)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_wide_allnull.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6549, 6)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_imp_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
